{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12aca72",
   "metadata": {},
   "source": [
    "# RNN results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c32785",
   "metadata": {},
   "source": [
    "# Without Cross validation and Without considering missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549de4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 167ms/step - accuracy: 0.9470 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.9136 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.9334 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.9241 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.9077 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.9327 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.9388 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.9417 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.9148 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.9179 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Accuracy on training data: 93.01%\n",
      "Accuracy on test data: 88.55%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jay\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes 0-1.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for RNN input (samples, time steps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = model.evaluate(X_train, y_train, verbose=0)[1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "\n",
    "print(f\"Accuracy on training data: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Accuracy on test data: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4ab85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed12fa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 158ms/step - accuracy: 0.9125 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 2/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.9371 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 3/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.9160 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 0.9233 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.9273 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.9277 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - accuracy: 0.9295 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 89ms/step - accuracy: 0.9442 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.9048 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - accuracy: 0.9406 - loss: nan - val_accuracy: 0.9359 - val_loss: nan\n",
      "Accuracy on training data: 93.01%\n",
      "Accuracy on test data: 88.55%\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 116ms/step\n",
      "Confusion Matrix:\n",
      "[[147   0]\n",
      " [ 19   0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jay\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes 0-1.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape data for RNN input (samples, time steps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the RNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "train_accuracy = model.evaluate(X_train, y_train, verbose=0)[1]\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_accuracy = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "\n",
    "print(f\"Accuracy on training data: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Accuracy on test data: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb31dd",
   "metadata": {},
   "source": [
    "# + Cross validation - considering missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c441fedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 [==============================] - 4s 48ms/step - loss: 0.5667 - accuracy: 0.9664\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 1s 47ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 1s 47ms/step - loss: 4.1375e-04 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 1s 47ms/step - loss: 1.6393e-04 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 1s 46ms/step - loss: 1.1881e-04 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 1s 47ms/step - loss: 1.0189e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 1s 46ms/step - loss: 9.2131e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 1s 46ms/step - loss: 8.4933e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 1s 46ms/step - loss: 7.9012e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 1s 47ms/step - loss: 7.3876e-05 - accuracy: 1.0000\n",
      "6/6 [==============================] - 1s 19ms/step\n",
      "13/13 [==============================] - 0s 21ms/step\n",
      "Accuracy on train data: 100.00%\n",
      "Accuracy on test data: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Exclude the last column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape data for RNN input (samples, time steps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the RNN model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the model\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "y_test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "y_train_pred = (model.predict(X_train) > 0.5).astype(int)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"Accuracy on train data: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Accuracy on test data: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67252e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "558283b0",
   "metadata": {},
   "source": [
    "# The optimal code with Cross-validation (without considering missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3d26fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 31ms/step\n",
      "2/2 [==============================] - 2s 27ms/step\n",
      "2/2 [==============================] - 1s 27ms/step\n",
      "2/2 [==============================] - 3s 35ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EAF6B108B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 32ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EAF1330280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 32ms/step\n",
      "2/2 [==============================] - 2s 32ms/step\n",
      "2/2 [==============================] - 2s 25ms/step\n",
      "2/2 [==============================] - 2s 25ms/step\n",
      "2/2 [==============================] - 2s 41ms/step\n",
      "Mean Accuracy: 91.67%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d31d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>145.118750</td>\n",
       "      <td>142.607292</td>\n",
       "      <td>136.781250</td>\n",
       "      <td>138.810417</td>\n",
       "      <td>138.488542</td>\n",
       "      <td>149.541667</td>\n",
       "      <td>151.360417</td>\n",
       "      <td>145.588542</td>\n",
       "      <td>136.220833</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>150.323958</td>\n",
       "      <td>138.341667</td>\n",
       "      <td>139.404167</td>\n",
       "      <td>148.260417</td>\n",
       "      <td>139.529167</td>\n",
       "      <td>161.725000</td>\n",
       "      <td>161.821875</td>\n",
       "      <td>151.493750</td>\n",
       "      <td>151.245833</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>143.712500</td>\n",
       "      <td>148.243750</td>\n",
       "      <td>146.097917</td>\n",
       "      <td>135.853125</td>\n",
       "      <td>141.971875</td>\n",
       "      <td>134.325000</td>\n",
       "      <td>135.838542</td>\n",
       "      <td>134.287500</td>\n",
       "      <td>134.361458</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>157.179167</td>\n",
       "      <td>156.259375</td>\n",
       "      <td>154.177083</td>\n",
       "      <td>143.554167</td>\n",
       "      <td>157.220833</td>\n",
       "      <td>131.486458</td>\n",
       "      <td>156.216667</td>\n",
       "      <td>155.942708</td>\n",
       "      <td>156.213542</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>2042</td>\n",
       "      <td>47.537500</td>\n",
       "      <td>141.019792</td>\n",
       "      <td>142.528125</td>\n",
       "      <td>142.020833</td>\n",
       "      <td>143.883333</td>\n",
       "      <td>102.576042</td>\n",
       "      <td>138.321875</td>\n",
       "      <td>122.770833</td>\n",
       "      <td>134.607292</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2043</td>\n",
       "      <td>55.040625</td>\n",
       "      <td>119.585417</td>\n",
       "      <td>111.237500</td>\n",
       "      <td>126.494792</td>\n",
       "      <td>69.259375</td>\n",
       "      <td>122.856250</td>\n",
       "      <td>99.140625</td>\n",
       "      <td>126.675000</td>\n",
       "      <td>111.025000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>2044</td>\n",
       "      <td>146.455208</td>\n",
       "      <td>156.293750</td>\n",
       "      <td>123.094792</td>\n",
       "      <td>150.250000</td>\n",
       "      <td>157.241667</td>\n",
       "      <td>165.626042</td>\n",
       "      <td>165.131250</td>\n",
       "      <td>166.731250</td>\n",
       "      <td>163.304167</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>2045</td>\n",
       "      <td>124.087500</td>\n",
       "      <td>116.191667</td>\n",
       "      <td>121.347917</td>\n",
       "      <td>112.247917</td>\n",
       "      <td>122.673958</td>\n",
       "      <td>123.755208</td>\n",
       "      <td>124.393750</td>\n",
       "      <td>126.363542</td>\n",
       "      <td>116.915625</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>2046</td>\n",
       "      <td>74.005208</td>\n",
       "      <td>78.928125</td>\n",
       "      <td>99.091667</td>\n",
       "      <td>66.653125</td>\n",
       "      <td>42.480208</td>\n",
       "      <td>108.951042</td>\n",
       "      <td>102.046875</td>\n",
       "      <td>115.465625</td>\n",
       "      <td>68.359375</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>552 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Patient           1           2           3           4           5  \\\n",
       "0       1001  145.118750  142.607292  136.781250  138.810417  138.488542   \n",
       "1       1002  150.323958  138.341667  139.404167  148.260417  139.529167   \n",
       "2       1003  143.712500  148.243750  146.097917  135.853125  141.971875   \n",
       "3       1004  157.179167  156.259375  154.177083  143.554167  157.220833   \n",
       "4       1005    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "..       ...         ...         ...         ...         ...         ...   \n",
       "547     2042   47.537500  141.019792  142.528125  142.020833  143.883333   \n",
       "548     2043   55.040625  119.585417  111.237500  126.494792   69.259375   \n",
       "549     2044  146.455208  156.293750  123.094792  150.250000  157.241667   \n",
       "550     2045  124.087500  116.191667  121.347917  112.247917  122.673958   \n",
       "551     2046   74.005208   78.928125   99.091667   66.653125   42.480208   \n",
       "\n",
       "              6           7           8           9  ...   82   83   84   85  \\\n",
       "0    149.541667  151.360417  145.588542  136.220833  ...  NaN  NaN  NaN  NaN   \n",
       "1    161.725000  161.821875  151.493750  151.245833  ...  NaN  NaN  NaN  NaN   \n",
       "2    134.325000  135.838542  134.287500  134.361458  ...  NaN  NaN  NaN  NaN   \n",
       "3    131.486458  156.216667  155.942708  156.213542  ...  NaN  NaN  NaN  NaN   \n",
       "4      0.000000    0.000000    0.000000    0.000000  ...  NaN  NaN  NaN  NaN   \n",
       "..          ...         ...         ...         ...  ...  ...  ...  ...  ...   \n",
       "547  102.576042  138.321875  122.770833  134.607292  ...  NaN  NaN  NaN  NaN   \n",
       "548  122.856250   99.140625  126.675000  111.025000  ...  0.0  0.0  0.0  0.0   \n",
       "549  165.626042  165.131250  166.731250  163.304167  ...  NaN  NaN  NaN  NaN   \n",
       "550  123.755208  124.393750  126.363542  116.915625  ...  NaN  NaN  NaN  NaN   \n",
       "551  108.951042  102.046875  115.465625   68.359375  ...  NaN  NaN  NaN  NaN   \n",
       "\n",
       "     86  87  88  89  90  label  \n",
       "0   NaN NaN NaN NaN NaN      0  \n",
       "1   NaN NaN NaN NaN NaN      0  \n",
       "2   NaN NaN NaN NaN NaN      0  \n",
       "3   NaN NaN NaN NaN NaN      0  \n",
       "4   NaN NaN NaN NaN NaN      0  \n",
       "..   ..  ..  ..  ..  ..    ...  \n",
       "547 NaN NaN NaN NaN NaN      1  \n",
       "548 NaN NaN NaN NaN NaN      1  \n",
       "549 NaN NaN NaN NaN NaN      1  \n",
       "550 NaN NaN NaN NaN NaN      1  \n",
       "551 NaN NaN NaN NaN NaN      1  \n",
       "\n",
       "[552 rows x 92 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0fcc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f85f92e2",
   "metadata": {},
   "source": [
    "# Considering missing values with mean strategy (+cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33dd5436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 20ms/step\n",
      "2/2 [==============================] - 1s 23ms/step\n",
      "2/2 [==============================] - 1s 24ms/step\n",
      "2/2 [==============================] - 1s 23ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002237FA619D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 22ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002231041A3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 26ms/step\n",
      "2/2 [==============================] - 1s 24ms/step\n",
      "2/2 [==============================] - 1s 25ms/step\n",
      "2/2 [==============================] - 1s 25ms/step\n",
      "2/2 [==============================] - 1s 29ms/step\n",
      "Mean Accuracy: 91.49%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Exclude the last column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f9a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb8cb13a",
   "metadata": {},
   "source": [
    "# With CV + Missing values with mean strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc2f7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 21ms/step\n",
      "2/2 [==============================] - 1s 22ms/step\n",
      "2/2 [==============================] - 1s 18ms/step\n",
      "2/2 [==============================] - 1s 22ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000231E960AA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 21ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000231FA18DA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 24ms/step\n",
      "2/2 [==============================] - 1s 23ms/step\n",
      "2/2 [==============================] - 1s 29ms/step\n",
      "2/2 [==============================] - 1s 25ms/step\n",
      "2/2 [==============================] - 1s 25ms/step\n",
      "Mean Accuracy: 92.03%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Exclude the last column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using median imputation (positive values)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f29397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001.0</td>\n",
       "      <td>145.118750</td>\n",
       "      <td>142.607292</td>\n",
       "      <td>136.781250</td>\n",
       "      <td>138.810417</td>\n",
       "      <td>138.488542</td>\n",
       "      <td>149.541667</td>\n",
       "      <td>151.360417</td>\n",
       "      <td>145.588542</td>\n",
       "      <td>136.220833</td>\n",
       "      <td>...</td>\n",
       "      <td>90.268229</td>\n",
       "      <td>90.580208</td>\n",
       "      <td>85.51875</td>\n",
       "      <td>75.407292</td>\n",
       "      <td>78.180208</td>\n",
       "      <td>72.880208</td>\n",
       "      <td>75.021875</td>\n",
       "      <td>62.307292</td>\n",
       "      <td>62.077083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002.0</td>\n",
       "      <td>150.323958</td>\n",
       "      <td>138.341667</td>\n",
       "      <td>139.404167</td>\n",
       "      <td>148.260417</td>\n",
       "      <td>139.529167</td>\n",
       "      <td>161.725000</td>\n",
       "      <td>161.821875</td>\n",
       "      <td>151.493750</td>\n",
       "      <td>151.245833</td>\n",
       "      <td>...</td>\n",
       "      <td>90.268229</td>\n",
       "      <td>90.580208</td>\n",
       "      <td>85.51875</td>\n",
       "      <td>75.407292</td>\n",
       "      <td>78.180208</td>\n",
       "      <td>72.880208</td>\n",
       "      <td>75.021875</td>\n",
       "      <td>62.307292</td>\n",
       "      <td>62.077083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003.0</td>\n",
       "      <td>143.712500</td>\n",
       "      <td>148.243750</td>\n",
       "      <td>146.097917</td>\n",
       "      <td>135.853125</td>\n",
       "      <td>141.971875</td>\n",
       "      <td>134.325000</td>\n",
       "      <td>135.838542</td>\n",
       "      <td>134.287500</td>\n",
       "      <td>134.361458</td>\n",
       "      <td>...</td>\n",
       "      <td>90.268229</td>\n",
       "      <td>90.580208</td>\n",
       "      <td>85.51875</td>\n",
       "      <td>75.407292</td>\n",
       "      <td>78.180208</td>\n",
       "      <td>72.880208</td>\n",
       "      <td>75.021875</td>\n",
       "      <td>62.307292</td>\n",
       "      <td>62.077083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004.0</td>\n",
       "      <td>157.179167</td>\n",
       "      <td>156.259375</td>\n",
       "      <td>154.177083</td>\n",
       "      <td>143.554167</td>\n",
       "      <td>157.220833</td>\n",
       "      <td>131.486458</td>\n",
       "      <td>156.216667</td>\n",
       "      <td>155.942708</td>\n",
       "      <td>156.213542</td>\n",
       "      <td>...</td>\n",
       "      <td>90.268229</td>\n",
       "      <td>90.580208</td>\n",
       "      <td>85.51875</td>\n",
       "      <td>75.407292</td>\n",
       "      <td>78.180208</td>\n",
       "      <td>72.880208</td>\n",
       "      <td>75.021875</td>\n",
       "      <td>62.307292</td>\n",
       "      <td>62.077083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005.0</td>\n",
       "      <td>134.291146</td>\n",
       "      <td>134.615625</td>\n",
       "      <td>135.044792</td>\n",
       "      <td>135.094792</td>\n",
       "      <td>133.101042</td>\n",
       "      <td>134.262500</td>\n",
       "      <td>133.883854</td>\n",
       "      <td>134.187500</td>\n",
       "      <td>134.628646</td>\n",
       "      <td>...</td>\n",
       "      <td>90.268229</td>\n",
       "      <td>90.580208</td>\n",
       "      <td>85.51875</td>\n",
       "      <td>75.407292</td>\n",
       "      <td>78.180208</td>\n",
       "      <td>72.880208</td>\n",
       "      <td>75.021875</td>\n",
       "      <td>62.307292</td>\n",
       "      <td>62.077083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>2042.0</td>\n",
       "      <td>47.537500</td>\n",
       "      <td>141.019792</td>\n",
       "      <td>142.528125</td>\n",
       "      <td>142.020833</td>\n",
       "      <td>143.883333</td>\n",
       "      <td>102.576042</td>\n",
       "      <td>138.321875</td>\n",
       "      <td>122.770833</td>\n",
       "      <td>134.607292</td>\n",
       "      <td>...</td>\n",
       "      <td>90.268229</td>\n",
       "      <td>90.580208</td>\n",
       "      <td>85.51875</td>\n",
       "      <td>75.407292</td>\n",
       "      <td>78.180208</td>\n",
       "      <td>72.880208</td>\n",
       "      <td>75.021875</td>\n",
       "      <td>62.307292</td>\n",
       "      <td>62.077083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2043.0</td>\n",
       "      <td>55.040625</td>\n",
       "      <td>119.585417</td>\n",
       "      <td>111.237500</td>\n",
       "      <td>126.494792</td>\n",
       "      <td>69.259375</td>\n",
       "      <td>122.856250</td>\n",
       "      <td>99.140625</td>\n",
       "      <td>126.675000</td>\n",
       "      <td>111.025000</td>\n",
       "      <td>...</td>\n",
       "      <td>90.268229</td>\n",
       "      <td>90.580208</td>\n",
       "      <td>85.51875</td>\n",
       "      <td>75.407292</td>\n",
       "      <td>78.180208</td>\n",
       "      <td>72.880208</td>\n",
       "      <td>75.021875</td>\n",
       "      <td>62.307292</td>\n",
       "      <td>62.077083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>2044.0</td>\n",
       "      <td>146.455208</td>\n",
       "      <td>156.293750</td>\n",
       "      <td>123.094792</td>\n",
       "      <td>150.250000</td>\n",
       "      <td>157.241667</td>\n",
       "      <td>165.626042</td>\n",
       "      <td>165.131250</td>\n",
       "      <td>166.731250</td>\n",
       "      <td>163.304167</td>\n",
       "      <td>...</td>\n",
       "      <td>90.268229</td>\n",
       "      <td>90.580208</td>\n",
       "      <td>85.51875</td>\n",
       "      <td>75.407292</td>\n",
       "      <td>78.180208</td>\n",
       "      <td>72.880208</td>\n",
       "      <td>75.021875</td>\n",
       "      <td>62.307292</td>\n",
       "      <td>62.077083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>2045.0</td>\n",
       "      <td>124.087500</td>\n",
       "      <td>116.191667</td>\n",
       "      <td>121.347917</td>\n",
       "      <td>112.247917</td>\n",
       "      <td>122.673958</td>\n",
       "      <td>123.755208</td>\n",
       "      <td>124.393750</td>\n",
       "      <td>126.363542</td>\n",
       "      <td>116.915625</td>\n",
       "      <td>...</td>\n",
       "      <td>90.268229</td>\n",
       "      <td>90.580208</td>\n",
       "      <td>85.51875</td>\n",
       "      <td>75.407292</td>\n",
       "      <td>78.180208</td>\n",
       "      <td>72.880208</td>\n",
       "      <td>75.021875</td>\n",
       "      <td>62.307292</td>\n",
       "      <td>62.077083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>2046.0</td>\n",
       "      <td>74.005208</td>\n",
       "      <td>78.928125</td>\n",
       "      <td>99.091667</td>\n",
       "      <td>66.653125</td>\n",
       "      <td>42.480208</td>\n",
       "      <td>108.951042</td>\n",
       "      <td>102.046875</td>\n",
       "      <td>115.465625</td>\n",
       "      <td>68.359375</td>\n",
       "      <td>...</td>\n",
       "      <td>90.268229</td>\n",
       "      <td>90.580208</td>\n",
       "      <td>85.51875</td>\n",
       "      <td>75.407292</td>\n",
       "      <td>78.180208</td>\n",
       "      <td>72.880208</td>\n",
       "      <td>75.021875</td>\n",
       "      <td>62.307292</td>\n",
       "      <td>62.077083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>552 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Patient           1           2           3           4           5  \\\n",
       "0     1001.0  145.118750  142.607292  136.781250  138.810417  138.488542   \n",
       "1     1002.0  150.323958  138.341667  139.404167  148.260417  139.529167   \n",
       "2     1003.0  143.712500  148.243750  146.097917  135.853125  141.971875   \n",
       "3     1004.0  157.179167  156.259375  154.177083  143.554167  157.220833   \n",
       "4     1005.0  134.291146  134.615625  135.044792  135.094792  133.101042   \n",
       "..       ...         ...         ...         ...         ...         ...   \n",
       "547   2042.0   47.537500  141.019792  142.528125  142.020833  143.883333   \n",
       "548   2043.0   55.040625  119.585417  111.237500  126.494792   69.259375   \n",
       "549   2044.0  146.455208  156.293750  123.094792  150.250000  157.241667   \n",
       "550   2045.0  124.087500  116.191667  121.347917  112.247917  122.673958   \n",
       "551   2046.0   74.005208   78.928125   99.091667   66.653125   42.480208   \n",
       "\n",
       "              6           7           8           9  ...         82  \\\n",
       "0    149.541667  151.360417  145.588542  136.220833  ...  90.268229   \n",
       "1    161.725000  161.821875  151.493750  151.245833  ...  90.268229   \n",
       "2    134.325000  135.838542  134.287500  134.361458  ...  90.268229   \n",
       "3    131.486458  156.216667  155.942708  156.213542  ...  90.268229   \n",
       "4    134.262500  133.883854  134.187500  134.628646  ...  90.268229   \n",
       "..          ...         ...         ...         ...  ...        ...   \n",
       "547  102.576042  138.321875  122.770833  134.607292  ...  90.268229   \n",
       "548  122.856250   99.140625  126.675000  111.025000  ...  90.268229   \n",
       "549  165.626042  165.131250  166.731250  163.304167  ...  90.268229   \n",
       "550  123.755208  124.393750  126.363542  116.915625  ...  90.268229   \n",
       "551  108.951042  102.046875  115.465625   68.359375  ...  90.268229   \n",
       "\n",
       "            83        84         85         86         87         88  \\\n",
       "0    90.580208  85.51875  75.407292  78.180208  72.880208  75.021875   \n",
       "1    90.580208  85.51875  75.407292  78.180208  72.880208  75.021875   \n",
       "2    90.580208  85.51875  75.407292  78.180208  72.880208  75.021875   \n",
       "3    90.580208  85.51875  75.407292  78.180208  72.880208  75.021875   \n",
       "4    90.580208  85.51875  75.407292  78.180208  72.880208  75.021875   \n",
       "..         ...       ...        ...        ...        ...        ...   \n",
       "547  90.580208  85.51875  75.407292  78.180208  72.880208  75.021875   \n",
       "548  90.580208  85.51875  75.407292  78.180208  72.880208  75.021875   \n",
       "549  90.580208  85.51875  75.407292  78.180208  72.880208  75.021875   \n",
       "550  90.580208  85.51875  75.407292  78.180208  72.880208  75.021875   \n",
       "551  90.580208  85.51875  75.407292  78.180208  72.880208  75.021875   \n",
       "\n",
       "            89         90  label  \n",
       "0    62.307292  62.077083      0  \n",
       "1    62.307292  62.077083      0  \n",
       "2    62.307292  62.077083      0  \n",
       "3    62.307292  62.077083      0  \n",
       "4    62.307292  62.077083      0  \n",
       "..         ...        ...    ...  \n",
       "547  62.307292  62.077083      1  \n",
       "548  62.307292  62.077083      1  \n",
       "549  62.307292  62.077083      1  \n",
       "550  62.307292  62.077083      1  \n",
       "551  62.307292  62.077083      1  \n",
       "\n",
       "[552 rows x 92 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee00ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e142395",
   "metadata": {},
   "source": [
    "# Considering missing values as meadian strategy ( without cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb9186b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "13/13 [==============================] - 4s 62ms/step - loss: 0.5977 - accuracy: 0.8782\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 1s 51ms/step - loss: 0.2692 - accuracy: 0.9301\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 1s 50ms/step - loss: 0.2481 - accuracy: 0.9301\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 1s 49ms/step - loss: 0.2371 - accuracy: 0.9301\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 1s 52ms/step - loss: 0.2329 - accuracy: 0.9301\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 1s 48ms/step - loss: 0.2212 - accuracy: 0.9301\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 1s 47ms/step - loss: 0.2410 - accuracy: 0.9275\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 1s 49ms/step - loss: 0.2370 - accuracy: 0.9301\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 1s 49ms/step - loss: 0.2210 - accuracy: 0.9301\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 1s 51ms/step - loss: 0.2167 - accuracy: 0.9301\n",
      "6/6 [==============================] - 1s 19ms/step\n",
      "13/13 [==============================] - 0s 19ms/step\n",
      "Accuracy on train data: 93.01%\n",
      "Accuracy on test data: 88.55%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Exclude the last column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using median imputation (positive values)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape data for RNN input (samples, time steps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the RNN model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the model\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "y_test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "y_train_pred = (model.predict(X_train) > 0.5).astype(int)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"Accuracy on train data: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Accuracy on test data: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05bbd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2abac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf75dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e1e5983",
   "metadata": {},
   "source": [
    "# Ecxperiments with NO-INFORMATION-CELLS as 0 values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533bbdd9",
   "metadata": {},
   "source": [
    "# with cv and no info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96bde21c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10300\\36102212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    926\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    927\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 928\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    929\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    930\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m     self._concrete_variable_creation_fn = (\n\u001b[1;32m--> 749\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m    \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    750\u001b[0m         ._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m    751\u001b[0m             *args, **kwds))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;34m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[0mconcrete_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcrete_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_get_concrete_function_internal_garbage_collected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeneralized_func_key\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_placeholder_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m           \u001b[0mconcrete_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m           \u001b[0mgraph_capture_container\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcrete_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_capture_func_lib\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    283\u001b[0m     concrete_function = monomorphic_function.ConcreteFunction(\n\u001b[1;32m--> 284\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m    285\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1323\u001b[0m         if x is not None)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1325\u001b[1;33m     \u001b[0mfunc_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    473\u001b[0m         \u001b[0mresource_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;31m# Deal with switches, finally.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Switch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m           self._process_switch(inp.op, ops_which_must_run,\n\u001b[0;32m    477\u001b[0m                                last_write_to_resource, merge_for_resource)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mtype\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2579\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2580\u001b[0m     \u001b[1;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2581\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2583\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Exclude the \"label\" column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4744b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbcbc60b",
   "metadata": {},
   "source": [
    "# Withot CV and No info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154bbb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Exclude the \"label\" column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Reshape data for RNN input (samples, time steps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the RNN model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the model\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "y_test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "y_train_pred = (model.predict(X_train) > 0.5).astype(int)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "print(f\"Accuracy on train data: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Accuracy on test data: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07663b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169e4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc6502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f801437",
   "metadata": {},
   "source": [
    "# With apgr1 as categorical labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30538d",
   "metadata": {},
   "source": [
    "# +CV + Missing value (mean strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c259d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 38ms/step\n",
      "2/2 [==============================] - 1s 44ms/step\n",
      "2/2 [==============================] - 1s 44ms/step\n",
      "2/2 [==============================] - 1s 47ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000010EE66983A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 53ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000010EE6641B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 56ms/step\n",
      "2/2 [==============================] - 1s 54ms/step\n",
      "2/2 [==============================] - 1s 53ms/step\n",
      "2/2 [==============================] - 1s 45ms/step\n",
      "2/2 [==============================] - 1s 96ms/step\n",
      "Mean Accuracy: 49.10%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Exclude the last column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using mean imputation (positive values)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Convert labels to binary (1 to 10 to 0 to 9)\n",
    "df['label'] = df['label'].apply(lambda x: x - 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model with improved architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Adjust output units for 10 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=0)  # Increase epochs and batch size\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)  # Convert softmax output to class labels\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e0675",
   "metadata": {},
   "source": [
    "**Results:** apgar1 outcome has nothing to do with FHR treands. 50% accuracy means having or not having FHR data doesn't make any difference. It is like you toss a coin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3a238",
   "metadata": {},
   "source": [
    "# +CV + No Missing value treatment (let 0 values be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b411367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 37ms/step\n",
      "2/2 [==============================] - 1s 36ms/step\n",
      "2/2 [==============================] - 1s 42ms/step\n",
      "2/2 [==============================] - 1s 52ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024A23E2A0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 53ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024A29507790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 64ms/step\n",
      "2/2 [==============================] - 1s 53ms/step\n",
      "2/2 [==============================] - 1s 58ms/step\n",
      "2/2 [==============================] - 1s 58ms/step\n",
      "2/2 [==============================] - 1s 60ms/step\n",
      "Mean Accuracy: 0.36%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 to 10 to 0 to 9)\n",
    "df['label'] = df['label'].apply(lambda x: x - 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model with improved architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Adjust output units for 10 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)  # Convert softmax output to class labels\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67857e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb663ba",
   "metadata": {},
   "source": [
    "# +CV + No Missing value treatment (0 values mean NO INFO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaf9fe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 39ms/step\n",
      "2/2 [==============================] - 1s 39ms/step\n",
      "2/2 [==============================] - 1s 44ms/step\n",
      "2/2 [==============================] - 1s 57ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017C363A4160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 54ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000017C35099CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 56ms/step\n",
      "2/2 [==============================] - 1s 50ms/step\n",
      "2/2 [==============================] - 1s 59ms/step\n",
      "2/2 [==============================] - 1s 55ms/step\n",
      "2/2 [==============================] - 1s 56ms/step\n",
      "Mean Accuracy: 0.36%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Replace 0 values with NaN to exclude them from calculations\n",
    "df[df != 0] = df[df != 0].replace(0, np.nan)\n",
    "\n",
    "# Convert labels to binary (1 to 10 to 0 to 9)\n",
    "df['label'] = df['label'].apply(lambda x: x - 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model with improved architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Adjust output units for 10 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=0)  # Increase epochs and batch size\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)  # Convert softmax output to class labels\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.nanmean(accuracies)  # Exclude NaN values\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ef5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd28f693",
   "metadata": {},
   "source": [
    "# With apgr5 as categorical labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60536a6e",
   "metadata": {},
   "source": [
    "# +CV + Missing value (mean strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa818ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 63ms/step\n",
      "2/2 [==============================] - 1s 70ms/step\n",
      "2/2 [==============================] - 1s 51ms/step\n",
      "2/2 [==============================] - 2s 205ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FF205F6EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 50ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001FF333FB700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 56ms/step\n",
      "2/2 [==============================] - 1s 66ms/step\n",
      "2/2 [==============================] - 1s 55ms/step\n",
      "2/2 [==============================] - 1s 47ms/step\n",
      "2/2 [==============================] - 1s 54ms/step\n",
      "Mean Accuracy: 43.40%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Exclude the last column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using mean imputation (positive values)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Convert labels to binary (1 to 10 to 0 to 9)\n",
    "df['label'] = df['label'].apply(lambda x: x - 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model with improved architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Adjust output units for 10 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=0)  # Increase epochs and batch size\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)  # Convert softmax output to class labels\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cb0985",
   "metadata": {},
   "source": [
    "**Results:** apgar5 outcome has nothing to do with FHR treands. The results is almost the same as apgar1. Achieving almost 50% accuracy means having or not having FHR data doesn't make any difference. It is like you toss a coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2403b2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001.000000</td>\n",
       "      <td>145.118750</td>\n",
       "      <td>142.607292</td>\n",
       "      <td>136.781250</td>\n",
       "      <td>138.810417</td>\n",
       "      <td>138.488542</td>\n",
       "      <td>149.541667</td>\n",
       "      <td>151.360417</td>\n",
       "      <td>145.588542</td>\n",
       "      <td>136.220833</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002.000000</td>\n",
       "      <td>150.323958</td>\n",
       "      <td>138.341667</td>\n",
       "      <td>139.404167</td>\n",
       "      <td>148.260417</td>\n",
       "      <td>139.529167</td>\n",
       "      <td>161.725000</td>\n",
       "      <td>161.821875</td>\n",
       "      <td>151.493750</td>\n",
       "      <td>151.245833</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003.000000</td>\n",
       "      <td>143.712500</td>\n",
       "      <td>148.243750</td>\n",
       "      <td>146.097917</td>\n",
       "      <td>135.853125</td>\n",
       "      <td>141.971875</td>\n",
       "      <td>134.325000</td>\n",
       "      <td>135.838542</td>\n",
       "      <td>134.287500</td>\n",
       "      <td>134.361458</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004.000000</td>\n",
       "      <td>157.179167</td>\n",
       "      <td>156.259375</td>\n",
       "      <td>154.177083</td>\n",
       "      <td>143.554167</td>\n",
       "      <td>157.220833</td>\n",
       "      <td>131.486458</td>\n",
       "      <td>156.216667</td>\n",
       "      <td>155.942708</td>\n",
       "      <td>156.213542</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005.000000</td>\n",
       "      <td>128.218641</td>\n",
       "      <td>130.512258</td>\n",
       "      <td>128.993490</td>\n",
       "      <td>129.726250</td>\n",
       "      <td>128.734919</td>\n",
       "      <td>128.729547</td>\n",
       "      <td>128.182656</td>\n",
       "      <td>129.147454</td>\n",
       "      <td>129.817015</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2043.000000</td>\n",
       "      <td>55.040625</td>\n",
       "      <td>119.585417</td>\n",
       "      <td>111.237500</td>\n",
       "      <td>126.494792</td>\n",
       "      <td>69.259375</td>\n",
       "      <td>122.856250</td>\n",
       "      <td>99.140625</td>\n",
       "      <td>126.675000</td>\n",
       "      <td>111.025000</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>2044.000000</td>\n",
       "      <td>146.455208</td>\n",
       "      <td>156.293750</td>\n",
       "      <td>123.094792</td>\n",
       "      <td>150.250000</td>\n",
       "      <td>157.241667</td>\n",
       "      <td>165.626042</td>\n",
       "      <td>165.131250</td>\n",
       "      <td>166.731250</td>\n",
       "      <td>163.304167</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>2045.000000</td>\n",
       "      <td>124.087500</td>\n",
       "      <td>116.191667</td>\n",
       "      <td>121.347917</td>\n",
       "      <td>112.247917</td>\n",
       "      <td>122.673958</td>\n",
       "      <td>123.755208</td>\n",
       "      <td>124.393750</td>\n",
       "      <td>126.363542</td>\n",
       "      <td>116.915625</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>2046.000000</td>\n",
       "      <td>74.005208</td>\n",
       "      <td>78.928125</td>\n",
       "      <td>99.091667</td>\n",
       "      <td>66.653125</td>\n",
       "      <td>42.480208</td>\n",
       "      <td>108.951042</td>\n",
       "      <td>102.046875</td>\n",
       "      <td>115.465625</td>\n",
       "      <td>68.359375</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>1317.666667</td>\n",
       "      <td>128.218641</td>\n",
       "      <td>130.512258</td>\n",
       "      <td>128.993490</td>\n",
       "      <td>129.726250</td>\n",
       "      <td>128.734919</td>\n",
       "      <td>128.729547</td>\n",
       "      <td>128.182656</td>\n",
       "      <td>129.147454</td>\n",
       "      <td>129.817015</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>553 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Patient           1           2           3           4           5  \\\n",
       "0    1001.000000  145.118750  142.607292  136.781250  138.810417  138.488542   \n",
       "1    1002.000000  150.323958  138.341667  139.404167  148.260417  139.529167   \n",
       "2    1003.000000  143.712500  148.243750  146.097917  135.853125  141.971875   \n",
       "3    1004.000000  157.179167  156.259375  154.177083  143.554167  157.220833   \n",
       "4    1005.000000  128.218641  130.512258  128.993490  129.726250  128.734919   \n",
       "..           ...         ...         ...         ...         ...         ...   \n",
       "548  2043.000000   55.040625  119.585417  111.237500  126.494792   69.259375   \n",
       "549  2044.000000  146.455208  156.293750  123.094792  150.250000  157.241667   \n",
       "550  2045.000000  124.087500  116.191667  121.347917  112.247917  122.673958   \n",
       "551  2046.000000   74.005208   78.928125   99.091667   66.653125   42.480208   \n",
       "552  1317.666667  128.218641  130.512258  128.993490  129.726250  128.734919   \n",
       "\n",
       "              6           7           8           9  ...         82  \\\n",
       "0    149.541667  151.360417  145.588542  136.220833  ...  85.711139   \n",
       "1    161.725000  161.821875  151.493750  151.245833  ...  85.711139   \n",
       "2    134.325000  135.838542  134.287500  134.361458  ...  85.711139   \n",
       "3    131.486458  156.216667  155.942708  156.213542  ...  85.711139   \n",
       "4    128.729547  128.182656  129.147454  129.817015  ...  85.711139   \n",
       "..          ...         ...         ...         ...  ...        ...   \n",
       "548  122.856250   99.140625  126.675000  111.025000  ...  85.711139   \n",
       "549  165.626042  165.131250  166.731250  163.304167  ...  85.711139   \n",
       "550  123.755208  124.393750  126.363542  116.915625  ...  85.711139   \n",
       "551  108.951042  102.046875  115.465625   68.359375  ...  85.711139   \n",
       "552  128.729547  128.182656  129.147454  129.817015  ...  85.711139   \n",
       "\n",
       "            83         84         85       86         87         88  \\\n",
       "0    84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "1    84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "2    84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "3    84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "4    84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "..         ...        ...        ...      ...        ...        ...   \n",
       "548  84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "549  84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "550  84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "551  84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "552  84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "\n",
       "            89         90  label  \n",
       "0    66.866414  68.349621      7  \n",
       "1    66.866414  68.349621      7  \n",
       "2    66.866414  68.349621      8  \n",
       "3    66.866414  68.349621      8  \n",
       "4    66.866414  68.349621      9  \n",
       "..         ...        ...    ...  \n",
       "548  66.866414  68.349621      8  \n",
       "549  66.866414  68.349621      7  \n",
       "550  66.866414  68.349621      8  \n",
       "551  66.866414  68.349621      6  \n",
       "552  66.866414  68.349621      3  \n",
       "\n",
       "[553 rows x 92 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93d5bdf",
   "metadata": {},
   "source": [
    "# - CV + Missing value (mean strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6c7fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 43ms/step\n",
      "4/4 [==============================] - 0s 40ms/step\n",
      "Training Accuracy: 36.43%\n",
      "Test Accuracy: 38.74%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Exclude the last column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using mean imputation (positive values)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Convert labels to binary (1 to 10 to 0 to 9)\n",
    "df['label'] = df['label'].apply(lambda x: x - 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model with improved architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Adjust output units for 10 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Split the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=0)  # Increase epochs and batch size\n",
    "\n",
    "# Training accuracy\n",
    "y_train_pred = np.argmax(model.predict(X_train), axis=-1)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Test accuracy\n",
    "y_test_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab14b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60866bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9abe9d4c",
   "metadata": {},
   "source": [
    "# +CV + Missing value (median strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630acbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 52ms/step\n",
      "2/2 [==============================] - 1s 50ms/step\n",
      "2/2 [==============================] - 1s 53ms/step\n",
      "2/2 [==============================] - 1s 53ms/step\n",
      "2/2 [==============================] - 1s 60ms/step\n",
      "2/2 [==============================] - 1s 53ms/step\n",
      "2/2 [==============================] - 1s 56ms/step\n",
      "2/2 [==============================] - 1s 56ms/step\n",
      "2/2 [==============================] - 1s 56ms/step\n",
      "2/2 [==============================] - 1s 55ms/step\n",
      "Mean Accuracy: 40.69%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Exclude the last column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using median imputation (positive values)\n",
    "imputer = SimpleImputer(strategy='median')  # Changed to median strategy\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Convert labels to binary (1 to 10 to 0 to 9)\n",
    "df['label'] = df['label'].apply(lambda x: x - 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model with improved architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Adjust output units for 10 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=0)  # Increase epochs and batch size\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)  # Convert softmax output to class labels\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75127a13",
   "metadata": {},
   "source": [
    "# +CV + No Missing value treatment (let 0 values be)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ebe2aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 18ms/step\n",
      "2/2 [==============================] - 1s 18ms/step\n",
      "2/2 [==============================] - 1s 19ms/step\n",
      "2/2 [==============================] - 1s 18ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002ADD2A6E820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 20ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002ADD6E5BA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 22ms/step\n",
      "2/2 [==============================] - 1s 19ms/step\n",
      "2/2 [==============================] - 1s 21ms/step\n",
      "2/2 [==============================] - 1s 20ms/step\n",
      "2/2 [==============================] - 1s 20ms/step\n",
      "Mean Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Convert labels to binary (1 to 10 to 0 to 9)\n",
    "df['label'] = df['label'].apply(lambda x: x - 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Adjust output units for 10 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)  # Convert softmax output to class labels\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.mean(accuracies)\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc73834c",
   "metadata": {},
   "source": [
    "# +CV + No Missing value treatment (0 values mean NO INFO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f021a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 40ms/step\n",
      "2/2 [==============================] - 1s 38ms/step\n",
      "2/2 [==============================] - 1s 44ms/step\n",
      "2/2 [==============================] - 1s 48ms/step\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002B673992160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 49ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002B672688CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 1s 56ms/step\n",
      "2/2 [==============================] - 1s 48ms/step\n",
      "2/2 [==============================] - 1s 55ms/step\n",
      "2/2 [==============================] - 1s 52ms/step\n",
      "2/2 [==============================] - 1s 55ms/step\n",
      "Mean Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Replace 0 values with NaN to exclude them from calculations\n",
    "df[df != 0] = df[df != 0].replace(0, np.nan)\n",
    "\n",
    "# Convert labels to binary (1 to 10 to 0 to 9)\n",
    "df['label'] = df['label'].apply(lambda x: x - 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model with improved architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')  # Adjust output units for 10 classes\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize cross-validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=0)  # Increase epochs and batch size\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)  # Convert softmax output to class labels\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Calculate and print the mean accuracy over all folds\n",
    "mean_accuracy = np.nanmean(accuracies)  # Exclude NaN values\n",
    "print(f\"Mean Accuracy: {mean_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0628ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f263cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753c92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77cbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d351fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf674fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c08b364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f24ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4840bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d76e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1f7577c",
   "metadata": {},
   "source": [
    "## Prediction accuracy on APGAR1 outcome (Binary encoding: 0-6 as 0 and 7-10 as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e258809f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 42ms/step\n",
      "Test Accuracy: 81.98%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Exclude the last column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using mean imputation (positive values)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Convert labels to binary (1 to 6 to 0 and 7 to 10 to 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x <= 6 else 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model with improved architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Output a single binary value\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=0)  # Increase epochs and batch size\n",
    "\n",
    "# Test accuracy\n",
    "y_test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7972bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbe36521",
   "metadata": {},
   "source": [
    "## Prediction dccuracy on APGAR5 outcome (Binary encoding: 0-6 as 0 and 7-10 as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7d7271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 77ms/step\n",
      "Test Accuracy: 99.10%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\MLFHRT\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Exclude the last column from zero value treatment\n",
    "cols_to_impute = df.columns[:-1]\n",
    "df[cols_to_impute] = df[cols_to_impute].replace(0, np.nan)\n",
    "\n",
    "# Impute missing values using mean imputation (positive values)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df[cols_to_impute] = imputer.fit_transform(df[cols_to_impute])\n",
    "\n",
    "# Convert labels to binary (1 to 6 to 0 and 7 to 10 to 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x <= 6 else 1)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X = df.iloc[:, 1:-1].values\n",
    "y = df['label'].values\n",
    "\n",
    "# Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define a function to create the RNN model with improved architecture\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, input_shape=(X.shape[1], 1)),\n",
    "        tf.keras.layers.LSTM(128),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Output a single binary value\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "model = create_model()\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, verbose=0)  # Increase epochs and batch size\n",
    "\n",
    "# Test accuracy\n",
    "y_test_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2974751e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001.000000</td>\n",
       "      <td>145.118750</td>\n",
       "      <td>142.607292</td>\n",
       "      <td>136.781250</td>\n",
       "      <td>138.810417</td>\n",
       "      <td>138.488542</td>\n",
       "      <td>149.541667</td>\n",
       "      <td>151.360417</td>\n",
       "      <td>145.588542</td>\n",
       "      <td>136.220833</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002.000000</td>\n",
       "      <td>150.323958</td>\n",
       "      <td>138.341667</td>\n",
       "      <td>139.404167</td>\n",
       "      <td>148.260417</td>\n",
       "      <td>139.529167</td>\n",
       "      <td>161.725000</td>\n",
       "      <td>161.821875</td>\n",
       "      <td>151.493750</td>\n",
       "      <td>151.245833</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003.000000</td>\n",
       "      <td>143.712500</td>\n",
       "      <td>148.243750</td>\n",
       "      <td>146.097917</td>\n",
       "      <td>135.853125</td>\n",
       "      <td>141.971875</td>\n",
       "      <td>134.325000</td>\n",
       "      <td>135.838542</td>\n",
       "      <td>134.287500</td>\n",
       "      <td>134.361458</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004.000000</td>\n",
       "      <td>157.179167</td>\n",
       "      <td>156.259375</td>\n",
       "      <td>154.177083</td>\n",
       "      <td>143.554167</td>\n",
       "      <td>157.220833</td>\n",
       "      <td>131.486458</td>\n",
       "      <td>156.216667</td>\n",
       "      <td>155.942708</td>\n",
       "      <td>156.213542</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005.000000</td>\n",
       "      <td>128.218641</td>\n",
       "      <td>130.512258</td>\n",
       "      <td>128.993490</td>\n",
       "      <td>129.726250</td>\n",
       "      <td>128.734919</td>\n",
       "      <td>128.729547</td>\n",
       "      <td>128.182656</td>\n",
       "      <td>129.147454</td>\n",
       "      <td>129.817015</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>2043.000000</td>\n",
       "      <td>55.040625</td>\n",
       "      <td>119.585417</td>\n",
       "      <td>111.237500</td>\n",
       "      <td>126.494792</td>\n",
       "      <td>69.259375</td>\n",
       "      <td>122.856250</td>\n",
       "      <td>99.140625</td>\n",
       "      <td>126.675000</td>\n",
       "      <td>111.025000</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>2044.000000</td>\n",
       "      <td>146.455208</td>\n",
       "      <td>156.293750</td>\n",
       "      <td>123.094792</td>\n",
       "      <td>150.250000</td>\n",
       "      <td>157.241667</td>\n",
       "      <td>165.626042</td>\n",
       "      <td>165.131250</td>\n",
       "      <td>166.731250</td>\n",
       "      <td>163.304167</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>2045.000000</td>\n",
       "      <td>124.087500</td>\n",
       "      <td>116.191667</td>\n",
       "      <td>121.347917</td>\n",
       "      <td>112.247917</td>\n",
       "      <td>122.673958</td>\n",
       "      <td>123.755208</td>\n",
       "      <td>124.393750</td>\n",
       "      <td>126.363542</td>\n",
       "      <td>116.915625</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>2046.000000</td>\n",
       "      <td>74.005208</td>\n",
       "      <td>78.928125</td>\n",
       "      <td>99.091667</td>\n",
       "      <td>66.653125</td>\n",
       "      <td>42.480208</td>\n",
       "      <td>108.951042</td>\n",
       "      <td>102.046875</td>\n",
       "      <td>115.465625</td>\n",
       "      <td>68.359375</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>1317.666667</td>\n",
       "      <td>128.218641</td>\n",
       "      <td>130.512258</td>\n",
       "      <td>128.993490</td>\n",
       "      <td>129.726250</td>\n",
       "      <td>128.734919</td>\n",
       "      <td>128.729547</td>\n",
       "      <td>128.182656</td>\n",
       "      <td>129.147454</td>\n",
       "      <td>129.817015</td>\n",
       "      <td>...</td>\n",
       "      <td>85.711139</td>\n",
       "      <td>84.493478</td>\n",
       "      <td>83.431677</td>\n",
       "      <td>76.781132</td>\n",
       "      <td>76.0516</td>\n",
       "      <td>73.467824</td>\n",
       "      <td>71.639732</td>\n",
       "      <td>66.866414</td>\n",
       "      <td>68.349621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>553 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Patient           1           2           3           4           5  \\\n",
       "0    1001.000000  145.118750  142.607292  136.781250  138.810417  138.488542   \n",
       "1    1002.000000  150.323958  138.341667  139.404167  148.260417  139.529167   \n",
       "2    1003.000000  143.712500  148.243750  146.097917  135.853125  141.971875   \n",
       "3    1004.000000  157.179167  156.259375  154.177083  143.554167  157.220833   \n",
       "4    1005.000000  128.218641  130.512258  128.993490  129.726250  128.734919   \n",
       "..           ...         ...         ...         ...         ...         ...   \n",
       "548  2043.000000   55.040625  119.585417  111.237500  126.494792   69.259375   \n",
       "549  2044.000000  146.455208  156.293750  123.094792  150.250000  157.241667   \n",
       "550  2045.000000  124.087500  116.191667  121.347917  112.247917  122.673958   \n",
       "551  2046.000000   74.005208   78.928125   99.091667   66.653125   42.480208   \n",
       "552  1317.666667  128.218641  130.512258  128.993490  129.726250  128.734919   \n",
       "\n",
       "              6           7           8           9  ...         82  \\\n",
       "0    149.541667  151.360417  145.588542  136.220833  ...  85.711139   \n",
       "1    161.725000  161.821875  151.493750  151.245833  ...  85.711139   \n",
       "2    134.325000  135.838542  134.287500  134.361458  ...  85.711139   \n",
       "3    131.486458  156.216667  155.942708  156.213542  ...  85.711139   \n",
       "4    128.729547  128.182656  129.147454  129.817015  ...  85.711139   \n",
       "..          ...         ...         ...         ...  ...        ...   \n",
       "548  122.856250   99.140625  126.675000  111.025000  ...  85.711139   \n",
       "549  165.626042  165.131250  166.731250  163.304167  ...  85.711139   \n",
       "550  123.755208  124.393750  126.363542  116.915625  ...  85.711139   \n",
       "551  108.951042  102.046875  115.465625   68.359375  ...  85.711139   \n",
       "552  128.729547  128.182656  129.147454  129.817015  ...  85.711139   \n",
       "\n",
       "            83         84         85       86         87         88  \\\n",
       "0    84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "1    84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "2    84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "3    84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "4    84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "..         ...        ...        ...      ...        ...        ...   \n",
       "548  84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "549  84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "550  84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "551  84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "552  84.493478  83.431677  76.781132  76.0516  73.467824  71.639732   \n",
       "\n",
       "            89         90  label  \n",
       "0    66.866414  68.349621      1  \n",
       "1    66.866414  68.349621      1  \n",
       "2    66.866414  68.349621      1  \n",
       "3    66.866414  68.349621      1  \n",
       "4    66.866414  68.349621      1  \n",
       "..         ...        ...    ...  \n",
       "548  66.866414  68.349621      1  \n",
       "549  66.866414  68.349621      1  \n",
       "550  66.866414  68.349621      1  \n",
       "551  66.866414  68.349621      1  \n",
       "552  66.866414  68.349621      0  \n",
       "\n",
       "[553 rows x 92 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581caee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

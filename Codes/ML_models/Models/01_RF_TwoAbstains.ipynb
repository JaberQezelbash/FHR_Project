{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50cfb34d-d919-406b-83f2-7e2658ec24ba",
   "metadata": {},
   "source": [
    "## Ranfom Forest - Final code - One Abstain\n",
    "\n",
    "   - We included FHR time searies **along with** OTHER FEATURES\n",
    "   - 10 Fold CV\n",
    "        - Within each fold, we considered 10 epochs while we established Early Stopping Method (with patience = 2)\n",
    "   - Oversampling (within each fold only on Test data)\n",
    "   - Abstain (Classiifcation with RFejection Option)\n",
    "        - Only one abstain parameter for both FN and FP\n",
    "        - Through each fold, we do different experiments based on different Abstains ranging from 0.50, 0.55,..., 0.90.\n",
    "            - We plot Confusion Matrix for each Fold and each abstain in that fold (i.e., 10*9=90 total confusion matices!)\n",
    "        - At the end of each Fold, the code gives two Tables and a Diagram on all Abstains.\n",
    "            - A plot that shows the trend and the Count of samples (TP, TN, FP, FN) that are abstained while increasing Abstain parameter\n",
    "            - A table for the name of those instances that are abstained.\n",
    "            - Another table that shows performance measures (Accuracy, Specificity, F1 Score, Recal, Precision) with respect to abstain values (i.e., based on Confusion Matrices).\n",
    "   - After all fold finished:\n",
    "        - The code reports the AVERAGE of all 10 folds performance measures.\n",
    "        - It draws the AVERAGE of all plots in all folds that shows the average trend and the Count of samples (TP, TN, FP, FN) that are abstained while increasing Abstain parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ce582-c910-4c80-9854-fc0ca98bf2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Step 1: Load Data from CSV file\n",
    "\n",
    "# Define the path to read the CSV file\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\Data_Results\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "\n",
    "# Define the directory path to save results\n",
    "save_directory = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\Codes\\ML_models\\Models\\All_Results\\RF_Results\\RF_FinalC:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\GitHub\\FHR_Project\\Codes\\ML_models\\Models\\All_Results\\RF_Results\\RF_Final\"\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Run Your Analysis Code\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Treat missing values with mean strategy\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X_time_series = df.iloc[:, :-15].values  # Exclude the last 15 columns\n",
    "X_additional_features = df[['Age', 'Gravidity', 'Sex', 'Parity', 'Hypertension', 'Diabetes', 'Preeclampsia',\n",
    "                             'Liq. praecox', 'Pyrexia', 'Meconium', 'Presentation', 'Induced', 'I.stage',\n",
    "                             'NoProgress', 'II.stage']].values\n",
    "y = df['label'].values  # Include the target variable y\n",
    "\n",
    "# Combine time series features and additional features\n",
    "X = np.concatenate([X_time_series, X_additional_features], axis=1)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the range of lambda values (excluding 0.95)\n",
    "lambdas = np.arange(0.5, 0.95, 0.05)\n",
    "\n",
    "# Initialize dictionaries to store metrics for each lambda\n",
    "metrics_dict = {l: {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []} for l in lambdas}\n",
    "\n",
    "# Function to classify based on probability and threshold\n",
    "def classify_with_reject(probabilities, threshold):\n",
    "    predictions = []\n",
    "    abstain_instances = []  # To store indices of abstain instances\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        if max(prob) >= threshold:\n",
    "            predictions.append(np.argmax(prob))  # Classify confident predictions\n",
    "        else:\n",
    "            predictions.append(-1)  # Reject classification for uncertain predictions\n",
    "            abstain_instances.append(i)  # Record abstain instance index\n",
    "    return np.array(predictions), abstain_instances\n",
    "\n",
    "# Perform K-Fold Cross Validation\n",
    "fold_number = 1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Extract time series and additional features for training\n",
    "    X_train_time_series = X_train[:, :-15]\n",
    "    X_train_additional = X_train[:, -15:]\n",
    "    \n",
    "    # Extract time series and additional features for testing\n",
    "    X_test_time_series = X_test[:, :-15]\n",
    "    X_test_additional = X_test[:, -15:]\n",
    "\n",
    "    # Standardize the features (mean=0, std=1)\n",
    "    scaler_time_series = StandardScaler()\n",
    "    X_train_time_series = scaler_time_series.fit_transform(X_train_time_series)\n",
    "    X_test_time_series = scaler_time_series.transform(X_test_time_series)\n",
    "\n",
    "    scaler_additional = StandardScaler()\n",
    "    X_train_additional = scaler_additional.fit_transform(X_train_additional)\n",
    "    X_test_additional = scaler_additional.transform(X_test_additional)\n",
    "\n",
    "    # Reshape data for Random Forest input (samples, features)\n",
    "    X_train = np.concatenate([X_train_time_series.reshape(X_train_time_series.shape[0], -1), X_train_additional], axis=1)\n",
    "    X_test = np.concatenate([X_test_time_series.reshape(X_test_time_series.shape[0], -1), X_test_additional], axis=1)\n",
    "\n",
    "    # Oversample the minority class using RandomOverSampler on training data\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train a Random Forest model with early stopping\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    no_improvement_epochs = 0\n",
    "    patience = 2  # Number of epochs with no improvement to wait before stopping early\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Evaluate on the training data\n",
    "        train_accuracy = accuracy_score(y_train_resampled, model.predict(X_train_resampled))\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if train_accuracy > best_score:\n",
    "            best_model = model\n",
    "            best_score = train_accuracy\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "        \n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Use the best model for predictions\n",
    "    test_probabilities = best_model.predict_proba(X_test)\n",
    "\n",
    "    # Initialize lists to store confusion matrix elements\n",
    "    tp_list = []\n",
    "    tn_list = []\n",
    "    fp_list = []\n",
    "    fn_list = []\n",
    "\n",
    "    # Initialize a table to store results for each lambda\n",
    "    table_data = []\n",
    "    abstain_table_data = []\n",
    "    metrics_table_data = []\n",
    "\n",
    "    # Loop through lambda values and calculate metrics\n",
    "    for reject_threshold in lambdas:\n",
    "        predictions, abstain_indices = classify_with_reject(test_probabilities, reject_threshold)\n",
    "        \n",
    "        # Filter out abstained instances\n",
    "        filtered_indices = [i for i in range(len(predictions)) if predictions[i] != -1]\n",
    "        y_test_filtered = y_test[filtered_indices]\n",
    "        predictions_filtered = predictions[filtered_indices]\n",
    "        \n",
    "        # Calculate confusion matrix elements\n",
    "        if len(predictions_filtered) > 0:\n",
    "            cm = confusion_matrix(y_test_filtered, predictions_filtered, labels=[0, 1])\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "        else:\n",
    "            cm = np.array([[0, 0], [0, 0]])\n",
    "            tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "        # Append confusion matrix elements to lists\n",
    "        tp_list.append(tp)\n",
    "        tn_list.append(tn)\n",
    "        fp_list.append(fp)\n",
    "        fn_list.append(fn)\n",
    "        \n",
    "        # Append data to table\n",
    "        table_data.append([round(reject_threshold, 2), tn, fp, fn, tp])\n",
    "        \n",
    "        # Report abstain instances (rows of data)\n",
    "        abstain_instances_info = []\n",
    "        for idx in abstain_indices:\n",
    "            abstain_instances_info.append((idx, y_test[idx]))\n",
    "        \n",
    "        abstain_table_data.append([round(reject_threshold, 2), abstain_instances_info])\n",
    "\n",
    "        # Calculate and store metrics\n",
    "        if len(y_test_filtered) > 0:\n",
    "            accuracy = accuracy_score(y_test_filtered, predictions_filtered) * 100\n",
    "            precision = precision_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            recall = recall_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            f1 = f1_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "            specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0\n",
    "        else:\n",
    "            accuracy = precision = recall = f1 = specificity = 0\n",
    "\n",
    "        metrics_dict[reject_threshold]['accuracy'].append(accuracy)\n",
    "        metrics_dict[reject_threshold]['precision'].append(precision)\n",
    "        metrics_dict[reject_threshold]['recall'].append(recall)\n",
    "        metrics_dict[reject_threshold]['f1'].append(f1)\n",
    "        metrics_dict[reject_threshold]['specificity'].append(specificity)\n",
    "        \n",
    "        metrics_table_data.append([round(reject_threshold, 2), f\"{accuracy:.2f}%\", f\"{precision:.2f}%\", f\"{recall:.2f}%\", f\"{f1:.2f}%\", f\"{specificity:.2f}%\"])\n",
    "\n",
    "        # Plot confusion matrix for this lambda\n",
    "        if cm.shape != (2, 2):\n",
    "            cm_padded = np.zeros((2, 2), dtype=int)\n",
    "            cm_padded[:cm.shape[0], :cm.shape[1]] = cm\n",
    "        else:\n",
    "            cm_padded = cm\n",
    "\n",
    "        x_labels = ['Normal', 'C-section']\n",
    "        y_labels = ['C-section', 'Normal']  # Reverse the order of y_labels\n",
    "        cm_reversed = cm_padded[::-1]\n",
    "        fig = ff.create_annotated_heatmap(z=cm_reversed, x=x_labels, y=y_labels, colorscale='Blues')\n",
    "        fig.update_layout(\n",
    "            title=f'Confusion Matrix, Fold {fold_number}, Lambda {reject_threshold:.2f}',\n",
    "            xaxis=dict(title='Predicted labels', tickfont=dict(size=10)),  # Adjust tick font size\n",
    "            yaxis=dict(title='True labels', tickfont=dict(size=10)),  # Adjust tick font size\n",
    "            width=400,  # Adjust width\n",
    "            height=300,  # Adjust height\n",
    "            margin=dict(l=50, r=50, t=130, b=50)  # Corrected margin specification\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    # Plot confusion matrix elements vs. lambda\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(lambdas, tp_list, marker='o', linestyle='-', label='True Positives (TP)')\n",
    "    plt.plot(lambdas, tn_list, marker='o', linestyle='-', label='True Negatives (TN)')\n",
    "    plt.plot(lambdas, fp_list, marker='o', linestyle='-', label='False Positives (FP)')\n",
    "    plt.plot(lambdas, fn_list, marker='o', linestyle='-', label='False Negatives (FN)')\n",
    "    plt.xlabel('Lambda (Abstain Threshold)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Confusion Matrix Elements vs. Lambda Threshold')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Create a DataFrame for the table and display it\n",
    "    df_table_cm = pd.DataFrame(table_data, columns=['Lambda Threshold', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)', 'True Positives (TP)'])\n",
    "    fig_table_cm = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_table_cm.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_table_cm[col].tolist() for col in df_table_cm.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_table_cm.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_table_cm.show()\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df_table_cm.to_excel(f'{save_directory}/Lambda_Abstain_Confusion_Matrix_Elements_Fold_{fold_number}.xlsx', index=False)\n",
    "  \n",
    "    # Create a DataFrame for abstain instances table and display it\n",
    "    df_abstain_table = pd.DataFrame(abstain_table_data, columns=['Lambda Threshold', 'Abstain Instances (Index, True Label)'])\n",
    "    fig_abstain_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_abstain_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_abstain_table[col].tolist() for col in df_abstain_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_abstain_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_abstain_table.show()\n",
    "\n",
    "    # Save the abstain instances DataFrame to an Excel file\n",
    "    df_abstain_table.to_excel(f'{save_directory}/Lambda_Abstain_Instances_Fold_{fold_number}.xlsx', index=False)\n",
    "    \n",
    "    # Create a DataFrame for the performance metrics table and display it\n",
    "    df_metrics_table = pd.DataFrame(metrics_table_data, columns=['Lambda Threshold', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity'])\n",
    "    fig_metrics_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_metrics_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_metrics_table[col].tolist() for col in df_metrics_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_metrics_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_metrics_table.show()\n",
    "\n",
    "    # Save the performance metrics DataFrame to an Excel file\n",
    "    #df_metrics_table.to_excel(f'Lambda_Abstain_Results_Metrics_Fold_{fold_number}.xlsx', index=False)\n",
    "    df_metrics_table.to_excel(f'{save_directory}/Lambda_Abstain_Results_Metrics_Fold_{fold_number}.xlsx', index=False)\n",
    "\n",
    "    fold_number += 1\n",
    "\n",
    "# Calculate average metrics for each lambda across all folds\n",
    "avg_metrics_data = []\n",
    "for l in lambdas:\n",
    "    avg_accuracy = np.mean(metrics_dict[l]['accuracy'])\n",
    "    avg_precision = np.mean(metrics_dict[l]['precision'])\n",
    "    avg_recall = np.mean(metrics_dict[l]['recall'])\n",
    "    avg_f1 = np.mean(metrics_dict[l]['f1'])\n",
    "    avg_specificity = np.mean(metrics_dict[l]['specificity'])\n",
    "    \n",
    "    avg_metrics_data.append([round(l, 2), f\"{avg_accuracy:.2f}%\", f\"{avg_precision:.2f}%\", f\"{avg_recall:.2f}%\", f\"{avg_f1:.2f}%\", f\"{avg_specificity:.2f}%\"])\n",
    "\n",
    "# Create a DataFrame for average metrics and display it\n",
    "df_avg_metrics = pd.DataFrame(avg_metrics_data, columns=['Lambda', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity'])\n",
    "fig_avg_metrics = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_avg_metrics.columns), fill_color='paleturquoise', align='left'),\n",
    "    cells=dict(values=[df_avg_metrics[col].tolist() for col in df_avg_metrics.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    ")])\n",
    "fig_avg_metrics.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "fig_avg_metrics.show()\n",
    "\n",
    "\n",
    "# Save the average metrics DataFrame to an Excel file\n",
    "df_avg_metrics.to_excel(f'{save_directory}/Average_Metrics_Per_Lambda.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Plot the average performance metrics vs. lambda\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Accuracy'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Accuracy')\n",
    "plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Precision'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Precision')\n",
    "plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Recall'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Recall')\n",
    "plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average F1-score'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average F1-score')\n",
    "plt.plot(df_avg_metrics['Lambda'], df_avg_metrics['Average Specificity'].str.rstrip('%').astype(float), marker='o', linestyle='-', label='Average Specificity')\n",
    "plt.xlabel('Lambda (Abstain Threshold)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Average Performance Metrics vs. Lambda Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage metrics for each lambda across all folds have been saved to 'Average_Metrics_Per_Lambda.xlsx'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8a4fb-c6ba-435b-af6a-e8d8686ecd73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56439a7f-4753-47d1-b3dc-ca41e964b4dd",
   "metadata": {},
   "source": [
    "# Final code with two abstains (more control on FN or FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543bba8-7611-4676-95ca-049d9c546380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Step 1: Load Data from CSV file\n",
    "# Define the path to read the CSV file\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\Data_Results\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "\n",
    "# Define the directory path to save results\n",
    "save_directory = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\GitHub\\FHR_Project\\Codes\\ML_models\\Models\\All_Results\\RF_Results\\TwoAbstains\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Run Your Analysis Code\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Treat missing values with mean strategy\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X_time_series = df.iloc[:, :-15].values  # Exclude the last 15 columns\n",
    "X_additional_features = df[['Age', 'Gravidity', 'Sex', 'Parity', 'Hypertension', 'Diabetes', 'Preeclampsia',\n",
    "                             'Liq. praecox', 'Pyrexia', 'Meconium', 'Presentation', 'Induced', 'I.stage',\n",
    "                             'NoProgress', 'II.stage']].values\n",
    "y = df['label'].values  # Include the target variable y\n",
    "\n",
    "# Combine time series features and additional features\n",
    "X = np.concatenate([X_time_series, X_additional_features], axis=1)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the range of lambda values\n",
    "lambda_range = np.arange(0.5, 0.95, 0.05)\n",
    "\n",
    "# Initialize dictionaries to store metrics for each combination of lambdas\n",
    "metrics_dict = {(fp, fn): {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []} for fp in lambda_range for fn in lambda_range}\n",
    "\n",
    "# Function to classify with dual reject thresholds\n",
    "def classify_with_dual_reject(probabilities, fp_threshold, fn_threshold):\n",
    "    predictions = []\n",
    "    abstain_instances = []  # Store indices of abstain instances\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        predicted_label = np.argmax(prob)\n",
    "        confidence = max(prob)\n",
    "        if (predicted_label == 1 and confidence < fp_threshold) or (predicted_label == 0 and confidence < fn_threshold):\n",
    "            predictions.append(-1)  # Abstain classification\n",
    "            abstain_instances.append(i)\n",
    "        else:\n",
    "            predictions.append(predicted_label)\n",
    "    return np.array(predictions), abstain_instances\n",
    "\n",
    "# Perform K-Fold Cross Validation\n",
    "fold_number = 1\n",
    "fp_threshold_to_remove_all_fp = None\n",
    "fn_threshold_to_remove_all_fn = None\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Extract time series and additional features for training\n",
    "    X_train_time_series = X_train[:, :-15]\n",
    "    X_train_additional = X_train[:, -15:]\n",
    "    \n",
    "    # Extract time series and additional features for testing\n",
    "    X_test_time_series = X_test[:, :-15]\n",
    "    X_test_additional = X_test[:, -15:]\n",
    "\n",
    "    # Standardize the features (mean=0, std=1)\n",
    "    scaler_time_series = StandardScaler()\n",
    "    X_train_time_series = scaler_time_series.fit_transform(X_train_time_series)\n",
    "    X_test_time_series = scaler_time_series.transform(X_test_time_series)\n",
    "\n",
    "    scaler_additional = StandardScaler()\n",
    "    X_train_additional = scaler_additional.fit_transform(X_train_additional)\n",
    "    X_test_additional = scaler_additional.transform(X_test_additional)\n",
    "\n",
    "    # Reshape data for Random Forest input (samples, features)\n",
    "    X_train = np.concatenate([X_train_time_series.reshape(X_train_time_series.shape[0], -1), X_train_additional], axis=1)\n",
    "    X_test = np.concatenate([X_test_time_series.reshape(X_test_time_series.shape[0], -1), X_test_additional], axis=1)\n",
    "\n",
    "    # Oversample the minority class using RandomOverSampler on training data\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train a Random Forest model with early stopping\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    no_improvement_epochs = 0\n",
    "    patience = 2  # Number of epochs with no improvement to wait before stopping early\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Evaluate on the training data\n",
    "        train_accuracy = accuracy_score(y_train_resampled, model.predict(X_train_resampled))\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if train_accuracy > best_score:\n",
    "            best_model = model\n",
    "            best_score = train_accuracy\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "        \n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Use the best model for predictions\n",
    "    test_probabilities = best_model.predict_proba(X_test)\n",
    "\n",
    "    # Initialize lists to store confusion matrix elements\n",
    "    tp_list = []\n",
    "    tn_list = []\n",
    "    fp_list = []\n",
    "    fn_list = []\n",
    "\n",
    "    # Initialize a table to store results for each lambda combination\n",
    "    table_data = []\n",
    "    abstain_table_data = []\n",
    "    metrics_table_data = []\n",
    "\n",
    "    # Loop through combinations of lambda values and calculate metrics\n",
    "    for fp_threshold in lambda_range:\n",
    "        for fn_threshold in lambda_range:\n",
    "            predictions, abstain_indices = classify_with_dual_reject(test_probabilities, fp_threshold, fn_threshold)\n",
    "        \n",
    "            # Filter out abstained instances\n",
    "            filtered_indices = [i for i in range(len(predictions)) if predictions[i] != -1]\n",
    "            y_test_filtered = y_test[filtered_indices]\n",
    "            predictions_filtered = predictions[filtered_indices]\n",
    "        \n",
    "            # Calculate confusion matrix elements\n",
    "            if len(predictions_filtered) > 0:\n",
    "                cm = confusion_matrix(y_test_filtered, predictions_filtered, labels=[0, 1])\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                cm = np.array([[0, 0], [0, 0]])\n",
    "                tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "            # Append confusion matrix elements to lists\n",
    "            tp_list.append(tp)\n",
    "            tn_list.append(tn)\n",
    "            fp_list.append(fp)\n",
    "            fn_list.append(fn)\n",
    "        \n",
    "            # Append data to table\n",
    "            table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), tn, fp, fn, tp])\n",
    "        \n",
    "            # Report abstain instances (rows of data)\n",
    "            abstain_instances_info = []\n",
    "            for idx in abstain_indices:\n",
    "                abstain_instances_info.append((idx, y_test[idx]))\n",
    "        \n",
    "            abstain_table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), abstain_instances_info])\n",
    "\n",
    "            # Calculate and store metrics\n",
    "            if len(y_test_filtered) > 0:\n",
    "                accuracy = accuracy_score(y_test_filtered, predictions_filtered) * 100\n",
    "                precision = precision_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "                recall = recall_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "                f1 = f1_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "                specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0\n",
    "            else:\n",
    "                accuracy = precision = recall = f1 = specificity = 0\n",
    "\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['accuracy'].append(accuracy)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['precision'].append(precision)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['recall'].append(recall)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['f1'].append(f1)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['specificity'].append(specificity)\n",
    "        \n",
    "            metrics_table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), f\"{accuracy:.2f}%\", f\"{precision:.2f}%\", f\"{recall:.2f}%\", f\"{f1:.2f}%\", f\"{specificity:.2f}%\"])\n",
    "\n",
    "            # Track thresholds for removing all FN and FP predictions\n",
    "            if fn == 0 and fn_threshold_to_remove_all_fn is None:\n",
    "                fn_threshold_to_remove_all_fn = fn_threshold\n",
    "\n",
    "            if fp == 0 and fp_threshold_to_remove_all_fp is None:\n",
    "                fp_threshold_to_remove_all_fp = fp_threshold\n",
    "\n",
    "            # Plot confusion matrix for this lambda combination\n",
    "            if cm.shape != (2, 2):\n",
    "                cm_padded = np.zeros((2, 2), dtype=int)\n",
    "                cm_padded[:cm.shape[0], :cm.shape[1]] = cm\n",
    "            else:\n",
    "                cm_padded = cm\n",
    "\n",
    "            x_labels = ['Normal', 'Abnormal']\n",
    "            y_labels = ['Abnormal', 'Normal']  # Reverse the order of y_labels\n",
    "            cm_reversed = cm_padded[::-1]\n",
    "            fig = ff.create_annotated_heatmap(z=cm_reversed, x=x_labels, y=y_labels, colorscale='Blues')\n",
    "            fig.update_layout(\n",
    "                title=f'Confusion Matrix, Fold {fold_number}, FP Lambda {fp_threshold:.2f}, FN Lambda {fn_threshold:.2f}',\n",
    "                xaxis=dict(title='Predicted labels', tickfont=dict(size=10)),  # Adjust tick font size\n",
    "                yaxis=dict(title='True labels', tickfont=dict(size=10)),  # Adjust tick font size\n",
    "                width=400,  # Adjust width\n",
    "                height=300,  # Adjust height\n",
    "                margin=dict(l=50, r=50, t=100, b=50)  # Corrected margin specification\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "    # Plot confusion matrix elements vs. lambda\n",
    "    for metric_list, label in zip([tp_list, tn_list, fp_list, fn_list], \n",
    "                                  ['True Positives (TP)', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)']):\n",
    "        fig = go.Figure()\n",
    "        for fp_threshold in lambda_range:\n",
    "            subset = [metric_list[i] for i in range(len(metric_list)) if table_data[i][0] == fp_threshold]\n",
    "            fig.add_trace(go.Scatter(x=lambda_range, y=subset, mode='lines+markers', name=f'FP Lambda={fp_threshold}'))\n",
    "        fig.update_layout(\n",
    "            title=f'Confusion Matrix Elements vs. FN Lambda Threshold ({label})',\n",
    "            xaxis_title='FN Lambda (Abstain Threshold)',\n",
    "            yaxis_title='Count',\n",
    "            width=800,\n",
    "            height=600\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    # Create a DataFrame for the table and display it\n",
    "    df_table_cm = pd.DataFrame(table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)', 'True Positives (TP)'])\n",
    "    fig_table_cm = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_table_cm.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_table_cm[col].tolist() for col in df_table_cm.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_table_cm.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_table_cm.show()\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df_table_cm.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Confusion_Matrix_Elements_Fold_{fold_number}.xlsx', index=False)\n",
    "  \n",
    "    # Create a DataFrame for abstain instances table and display it\n",
    "    df_abstain_table = pd.DataFrame(abstain_table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'Abstain Instances (Index, True Label)'])\n",
    "    fig_abstain_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_abstain_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_abstain_table[col].tolist() for col in df_abstain_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_abstain_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_abstain_table.show()\n",
    "\n",
    "    # Save the abstain instances DataFrame to an Excel file\n",
    "    df_abstain_table.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Instances_Fold_{fold_number}.xlsx', index=False)\n",
    "    \n",
    "    # Create a DataFrame for the performance metrics table and display it\n",
    "    df_metrics_table = pd.DataFrame(metrics_table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity'])\n",
    "    fig_metrics_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_metrics_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_metrics_table[col].tolist() for col in df_metrics_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_metrics_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_metrics_table.show()\n",
    "\n",
    "    # Save the performance metrics DataFrame to an Excel file\n",
    "    df_metrics_table.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Results_Metrics_Fold_{fold_number}.xlsx', index=False)\n",
    "\n",
    "    fold_number += 1\n",
    "\n",
    "# Calculate average metrics for each lambda combination across all folds\n",
    "avg_metrics_data = []\n",
    "for fp in lambda_range:\n",
    "    for fn in lambda_range:\n",
    "        avg_accuracy = np.mean(metrics_dict[(fp, fn)]['accuracy'])\n",
    "        avg_precision = np.mean(metrics_dict[(fp, fn)]['precision'])\n",
    "        avg_recall = np.mean(metrics_dict[(fp, fn)]['recall'])\n",
    "        avg_f1 = np.mean(metrics_dict[(fp, fn)]['f1'])\n",
    "        avg_specificity = np.mean(metrics_dict[(fp, fn)]['specificity'])\n",
    "    \n",
    "        avg_metrics_data.append([round(fp, 2), round(fn, 2), f\"{avg_accuracy:.2f}%\", f\"{avg_precision:.2f}%\", f\"{avg_recall:.2f}%\", f\"{avg_f1:.2f}%\", f\"{avg_specificity:.2f}%\"])\n",
    "\n",
    "# Create a DataFrame for average metrics and display it\n",
    "df_avg_metrics = pd.DataFrame(avg_metrics_data, columns=['FP Lambda', 'FN Lambda', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity'])\n",
    "fig_avg_metrics = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_avg_metrics.columns), fill_color='paleturquoise', align='left'),\n",
    "    cells=dict(values=[df_avg_metrics[col].tolist() for col in df_avg_metrics.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    ")])\n",
    "fig_avg_metrics.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "fig_avg_metrics.show()\n",
    "\n",
    "# Save the average metrics DataFrame to an Excel file\n",
    "df_avg_metrics.to_excel(f'{save_directory}/Average_Dual_Lambda_Metrics.xlsx', index=False)\n",
    "\n",
    "# Plot the average performance metrics vs. lambda\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Accuracy']], marker='o', linestyle='-', label='Average Accuracy')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Precision']], marker='o', linestyle='-', label='Average Precision')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Recall']], marker='o', linestyle='-', label='Average Recall')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average F1-score']], marker='o', linestyle='-', label='Average F1-score')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Specificity']], marker='o', linestyle='-', label='Average Specificity')\n",
    "plt.xlabel('FP Lambda (Abstain Threshold)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Average Performance Metrics vs. FP Lambda Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage metrics for each combination of FP and FN lambda across all folds have been saved to 'Average_Dual_Lambda_Metrics.xlsx'.\")\n",
    "print(f\"\\nFP threshold to remove all FP predictions: {fp_threshold_to_remove_all_fp}\")\n",
    "print(f\"FN threshold to remove all FN predictions: {fn_threshold_to_remove_all_fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93aa274-e073-416b-9239-ab42dab1db0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443c569f-377d-4b21-a323-cebb94e94479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Step 1: Load Data from CSV file\n",
    "\n",
    "# Define the path to read the CSV file\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\Data_Results\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "\n",
    "# Define the directory path to save results\n",
    "save_directory = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\GitHub\\FHR_Project\\Codes\\ML_models\\Models\\All_Results\\RF_Results\\TwoAbstains\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Run Your Analysis Code\n",
    "# Convert labels to binary (1 or 2 to 0 or 1)\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "\n",
    "# Treat missing values with mean strategy\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Extract features (FHR time series) and labels\n",
    "X_time_series = df.iloc[:, :-15].values  # Exclude the last 15 columns\n",
    "X_additional_features = df[['Age', 'Gravidity', 'Sex', 'Parity', 'Hypertension', 'Diabetes', 'Preeclampsia',\n",
    "                             'Liq. praecox', 'Pyrexia', 'Meconium', 'Presentation', 'Induced', 'I.stage',\n",
    "                             'NoProgress', 'II.stage']].values\n",
    "y = df['label'].values  # Include the target variable y\n",
    "\n",
    "# Combine time series features and additional features\n",
    "X = np.concatenate([X_time_series, X_additional_features], axis=1)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the range of lambda values\n",
    "lambda_range = np.arange(0.5, 0.95, 0.05)\n",
    "\n",
    "# Initialize dictionaries to store metrics for each combination of lambdas\n",
    "metrics_dict = {(fp, fn): {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []} for fp in lambda_range for fn in lambda_range}\n",
    "\n",
    "# Function to classify with dual reject thresholds\n",
    "def classify_with_dual_reject(probabilities, fp_threshold, fn_threshold):\n",
    "    predictions = []\n",
    "    abstain_instances = []  # Store indices of abstain instances\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        predicted_label = np.argmax(prob)\n",
    "        confidence = max(prob)\n",
    "        if (predicted_label == 1 and confidence < fp_threshold) or (predicted_label == 0 and confidence < fn_threshold):\n",
    "            predictions.append(-1)  # Abstain classification\n",
    "            abstain_instances.append(i)\n",
    "        else:\n",
    "            predictions.append(predicted_label)\n",
    "    return np.array(predictions), abstain_instances\n",
    "\n",
    "# Perform K-Fold Cross Validation\n",
    "fold_number = 1\n",
    "fp_threshold_to_remove_all_fp = None\n",
    "fn_threshold_to_remove_all_fn = None\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Extract time series and additional features for training\n",
    "    X_train_time_series = X_train[:, :-15]\n",
    "    X_train_additional = X_train[:, -15:]\n",
    "    \n",
    "    # Extract time series and additional features for testing\n",
    "    X_test_time_series = X_test[:, :-15]\n",
    "    X_test_additional = X_test[:, -15:]\n",
    "\n",
    "    # Standardize the features (mean=0, std=1)\n",
    "    scaler_time_series = StandardScaler()\n",
    "    X_train_time_series = scaler_time_series.fit_transform(X_train_time_series)\n",
    "    X_test_time_series = scaler_time_series.transform(X_test_time_series)\n",
    "\n",
    "    scaler_additional = StandardScaler()\n",
    "    X_train_additional = scaler_additional.fit_transform(X_train_additional)\n",
    "    X_test_additional = scaler_additional.transform(X_test_additional)\n",
    "\n",
    "    # Reshape data for Random Forest input (samples, features)\n",
    "    X_train = np.concatenate([X_train_time_series.reshape(X_train_time_series.shape[0], -1), X_train_additional], axis=1)\n",
    "    X_test = np.concatenate([X_test_time_series.reshape(X_test_time_series.shape[0], -1), X_test_additional], axis=1)\n",
    "\n",
    "    # Oversample the minority class using RandomOverSampler on training data\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Train a Random Forest model with early stopping\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    no_improvement_epochs = 0\n",
    "    patience = 2  # Number of epochs with no improvement to wait before stopping early\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Evaluate on the training data\n",
    "        train_accuracy = accuracy_score(y_train_resampled, model.predict(X_train_resampled))\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if train_accuracy > best_score:\n",
    "            best_model = model\n",
    "            best_score = train_accuracy\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "        \n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Use the best model for predictions\n",
    "    test_probabilities = best_model.predict_proba(X_test)\n",
    "\n",
    "    # Initialize lists to store confusion matrix elements\n",
    "    tp_list = []\n",
    "    tn_list = []\n",
    "    fp_list = []\n",
    "    fn_list = []\n",
    "\n",
    "    # Initialize a table to store results for each lambda combination\n",
    "    table_data = []\n",
    "    abstain_table_data = []\n",
    "    metrics_table_data = []\n",
    "\n",
    "    # Loop through combinations of lambda values and calculate metrics\n",
    "    for fp_threshold in lambda_range:\n",
    "        for fn_threshold in lambda_range:\n",
    "            predictions, abstain_indices = classify_with_dual_reject(test_probabilities, fp_threshold, fn_threshold)\n",
    "        \n",
    "            # Filter out abstained instances\n",
    "            filtered_indices = [i for i in range(len(predictions)) if predictions[i] != -1]\n",
    "            y_test_filtered = y_test[filtered_indices]\n",
    "            predictions_filtered = predictions[filtered_indices]\n",
    "        \n",
    "            # Calculate confusion matrix elements\n",
    "            if len(predictions_filtered) > 0:\n",
    "                cm = confusion_matrix(y_test_filtered, predictions_filtered, labels=[0, 1])\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                cm = np.array([[0, 0], [0, 0]])\n",
    "                tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "            # Append confusion matrix elements to lists\n",
    "            tp_list.append(tp)\n",
    "            tn_list.append(tn)\n",
    "            fp_list.append(fp)\n",
    "            fn_list.append(fn)\n",
    "        \n",
    "            # Append data to table\n",
    "            table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), tn, fp, fn, tp])\n",
    "        \n",
    "            # Report abstain instances (rows of data)\n",
    "            abstain_instances_info = []\n",
    "            for idx in abstain_indices:\n",
    "                abstain_instances_info.append((idx, y_test[idx]))\n",
    "        \n",
    "            abstain_table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), abstain_instances_info])\n",
    "\n",
    "            # Calculate and store metrics\n",
    "            if len(y_test_filtered) > 0:\n",
    "                accuracy = accuracy_score(y_test_filtered, predictions_filtered) * 100\n",
    "                precision = precision_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "                recall = recall_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "                f1 = f1_score(y_test_filtered, predictions_filtered, zero_division=0) * 100\n",
    "                specificity = (tn / (tn + fp)) * 100 if (tn + fp) > 0 else 0\n",
    "            else:\n",
    "                accuracy = precision = recall = f1 = specificity = 0\n",
    "\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['accuracy'].append(accuracy)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['precision'].append(precision)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['recall'].append(recall)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['f1'].append(f1)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['specificity'].append(specificity)\n",
    "        \n",
    "            metrics_table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), f\"{accuracy:.2f}%\", f\"{precision:.2f}%\", f\"{recall:.2f}%\", f\"{f1:.2f}%\", f\"{specificity:.2f}%\"])\n",
    "\n",
    "            # Track thresholds for removing all FN and FP predictions\n",
    "            if fn == 0 and fn_threshold_to_remove_all_fn is None:\n",
    "                fn_threshold_to_remove_all_fn = fn_threshold\n",
    "\n",
    "            if fp == 0 and fp_threshold_to_remove_all_fp is None:\n",
    "                fp_threshold_to_remove_all_fp = fp_threshold\n",
    "\n",
    "            # Plot confusion matrix for this lambda combination\n",
    "            if cm.shape != (2, 2):\n",
    "                cm_padded = np.zeros((2, 2), dtype=int)\n",
    "                cm_padded[:cm.shape[0], :cm.shape[1]] = cm\n",
    "            else:\n",
    "                cm_padded = cm\n",
    "\n",
    "            x_labels = ['Normal', 'Abnormal']\n",
    "            y_labels = ['Abnormal', 'Normal']  # Reverse the order of y_labels\n",
    "            cm_reversed = cm_padded[::-1]\n",
    "            fig = ff.create_annotated_heatmap(z=cm_reversed, x=x_labels, y=y_labels, colorscale='Blues')\n",
    "            fig.update_layout(\n",
    "                title=f'Confusion Matrix, Fold {fold_number}, FP Lambda {fp_threshold:.2f}, FN Lambda {fn_threshold:.2f}',\n",
    "                xaxis=dict(title='Predicted labels', tickfont=dict(size=10)),  # Adjust tick font size\n",
    "                yaxis=dict(title='True labels', tickfont=dict(size=10)),  # Adjust tick font size\n",
    "                width=400,  # Adjust width\n",
    "                height=300,  # Adjust height\n",
    "                margin=dict(l=50, r=50, t=100, b=50)  # Corrected margin specification\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "    # Plot confusion matrix elements vs. lambda\n",
    "    for metric_list, label in zip([tp_list, tn_list, fp_list, fn_list], \n",
    "                                  ['True Positives (TP)', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)']):\n",
    "        fig = go.Figure()\n",
    "        for fp_threshold in lambda_range:\n",
    "            subset = [metric_list[i] for i in range(len(metric_list)) if table_data[i][0] == fp_threshold]\n",
    "            fig.add_trace(go.Scatter(x=lambda_range, y=subset, mode='lines+markers', name=f'FP Lambda={fp_threshold}'))\n",
    "        fig.update_layout(\n",
    "            title=f'Confusion Matrix Elements vs. FN Lambda Threshold ({label})',\n",
    "            xaxis_title='FN Lambda (Abstain Threshold)',\n",
    "            yaxis_title='Count',\n",
    "            width=800,\n",
    "            height=600\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    # Create a DataFrame for the table and display it\n",
    "    df_table_cm = pd.DataFrame(table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)', 'True Positives (TP)'])\n",
    "    fig_table_cm = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_table_cm.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_table_cm[col].tolist() for col in df_table_cm.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_table_cm.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_table_cm.show()\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df_table_cm.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Confusion_Matrix_Elements_Fold_{fold_number}.xlsx', index=False)\n",
    "  \n",
    "    # Create a DataFrame for abstain instances table and display it\n",
    "    df_abstain_table = pd.DataFrame(abstain_table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'Abstain Instances (Index, True Label)'])\n",
    "    fig_abstain_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_abstain_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_abstain_table[col].tolist() for col in df_abstain_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_abstain_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_abstain_table.show()\n",
    "\n",
    "    # Save the abstain instances DataFrame to an Excel file\n",
    "    df_abstain_table.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Instances_Fold_{fold_number}.xlsx', index=False)\n",
    "    \n",
    "    # Create a DataFrame for the performance metrics table and display it\n",
    "    df_metrics_table = pd.DataFrame(metrics_table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity'])\n",
    "    fig_metrics_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_metrics_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_metrics_table[col].tolist() for col in df_metrics_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_metrics_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_metrics_table.show()\n",
    "\n",
    "    # Save the performance metrics DataFrame to an Excel file\n",
    "    df_metrics_table.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Results_Metrics_Fold_{fold_number}.xlsx', index=False)\n",
    "\n",
    "    fold_number += 1\n",
    "\n",
    "# Calculate average metrics for each lambda combination across all folds\n",
    "avg_metrics_data = []\n",
    "for fp in lambda_range:\n",
    "    for fn in lambda_range:\n",
    "        avg_accuracy = np.mean(metrics_dict[(fp, fn)]['accuracy'])\n",
    "        avg_precision = np.mean(metrics_dict[(fp, fn)]['precision'])\n",
    "        avg_recall = np.mean(metrics_dict[(fp, fn)]['recall'])\n",
    "        avg_f1 = np.mean(metrics_dict[(fp, fn)]['f1'])\n",
    "        avg_specificity = np.mean(metrics_dict[(fp, fn)]['specificity'])\n",
    "    \n",
    "        avg_metrics_data.append([round(fp, 2), round(fn, 2), f\"{avg_accuracy:.2f}%\", f\"{avg_precision:.2f}%\", f\"{avg_recall:.2f}%\", f\"{avg_f1:.2f}%\", f\"{avg_specificity:.2f}%\"])\n",
    "\n",
    "# Create a DataFrame for average metrics and display it\n",
    "df_avg_metrics = pd.DataFrame(avg_metrics_data, columns=['FP Lambda', 'FN Lambda', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity'])\n",
    "fig_avg_metrics = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_avg_metrics.columns), fill_color='paleturquoise', align='left'),\n",
    "    cells=dict(values=[df_avg_metrics[col].tolist() for col in df_avg_metrics.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    ")])\n",
    "fig_avg_metrics.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "fig_avg_metrics.show()\n",
    "\n",
    "# Save the average metrics DataFrame to an Excel file\n",
    "df_avg_metrics.to_excel(f'{save_directory}/Average_Dual_Lambda_Metrics.xlsx', index=False)\n",
    "\n",
    "# Extract unique FP and FN lambda values for plotting\n",
    "unique_fp_lambdas = df_avg_metrics['FP Lambda'].unique()\n",
    "unique_fn_lambdas = df_avg_metrics['FN Lambda'].unique()\n",
    "\n",
    "# Prepare data for 3D surface plot\n",
    "z_values = df_avg_metrics['Average Accuracy'].str.rstrip('%').astype(float).values.reshape(len(unique_fp_lambdas), len(unique_fn_lambdas))\n",
    "\n",
    "# Create a 3D surface plot using Plotly\n",
    "fig = go.Figure(data=[go.Surface(z=z_values, x=unique_fp_lambdas, y=unique_fn_lambdas, colorscale='Viridis')])\n",
    "fig.update_layout(title='3D Plot of Average Accuracy vs. FP and FN Lambda Thresholds',\n",
    "                  scene=dict(xaxis_title='FP Lambda Threshold', yaxis_title='FN Lambda Threshold', zaxis_title='Average Accuracy'),\n",
    "                  autosize=False, width=800, height=800)\n",
    "fig.show()\n",
    "\n",
    "# Plot the average performance metrics vs. lambda\n",
    "plt.figure(figsize=(10, 6))\n",
    "for metric, label in zip(['Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity'], \n",
    "                         ['Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity']):\n",
    "    plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics[metric]], marker='o', linestyle='-', label=label)\n",
    "plt.xlabel('FP Lambda (Abstain Threshold)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Average Performance Metrics vs. FP Lambda Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage metrics for each combination of FP and FN lambda across all folds have been saved to 'Average_Dual_Lambda_Metrics.xlsx'.\")\n",
    "print(f\"\\nFP threshold to remove all FP predictions: {fp_threshold_to_remove_all_fp}\")\n",
    "print(f\"FN threshold to remove all FN predictions: {fn_threshold_to_remove_all_fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a8512-a78d-43dc-bba3-e1f1039db528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8839e4-4877-45d8-8fb9-957bc84247b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de5dea-e388-4ca8-88f6-1749da3884f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd7bad-3d7f-477f-aa0f-8044261c3ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a8dd1-40b8-43fb-a092-e9d59f935739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

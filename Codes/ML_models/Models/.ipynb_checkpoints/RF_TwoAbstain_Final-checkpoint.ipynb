{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0052923-d2e0-41a1-9d20-3f02f09043f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8446d5-5114-4f5e-bdd7-0693b33dfc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Step 1: Load Data from CSV file\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\Data_Results\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "save_directory = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\GitHub\\FHR_Project\\Codes\\ML_models\\Models\\All_Results\\RF_Results\\TwoAbstains\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Run Your Analysis Code\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "X_time_series = df.iloc[:, :-15].values\n",
    "X_additional_features = df[['Age', 'Gravidity', 'Sex', 'Parity', 'Hypertension', 'Diabetes', 'Preeclampsia',\n",
    "                             'Liq. praecox', 'Pyrexia', 'Meconium', 'Presentation', 'Induced', 'I.stage',\n",
    "                             'NoProgress', 'II.stage']].values\n",
    "y = df['label'].values\n",
    "X = np.concatenate([X_time_series, X_additional_features], axis=1)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "lambda_range = np.arange(0.5, 0.95, 0.05)\n",
    "metrics_dict = {(fp, fn): {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []} for fp in lambda_range for fn in lambda_range}\n",
    "\n",
    "def classify_with_dual_reject(probabilities, fp_threshold, fn_threshold):\n",
    "    predictions = []\n",
    "    abstain_instances = []\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        predicted_label = np.argmax(prob)\n",
    "        confidence = max(prob)\n",
    "        if (predicted_label == 1 and confidence < fp_threshold) or (predicted_label == 0 and confidence < fn_threshold):\n",
    "            predictions.append(-1)\n",
    "            abstain_instances.append(i)\n",
    "        else:\n",
    "            predictions.append(predicted_label)\n",
    "    return np.array(predictions), abstain_instances\n",
    "\n",
    "fold_number = 1\n",
    "fp_threshold_to_remove_all_fp = None\n",
    "fn_threshold_to_remove_all_fn = None\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train_time_series = X_train[:, :-15]\n",
    "    X_train_additional = X_train[:, -15:]\n",
    "    X_test_time_series = X_test[:, :-15]\n",
    "    X_test_additional = X_test[:, -15:]\n",
    "\n",
    "    scaler_time_series = StandardScaler()\n",
    "    X_train_time_series = scaler_time_series.fit_transform(X_train_time_series)\n",
    "    X_test_time_series = scaler_time_series.transform(X_test_time_series)\n",
    "    scaler_additional = StandardScaler()\n",
    "    X_train_additional = scaler_additional.fit_transform(X_train_additional)\n",
    "    X_test_additional = scaler_additional.transform(X_test_additional)\n",
    "\n",
    "    X_train = np.concatenate([X_train_time_series, X_train_additional], axis=1)\n",
    "    X_test = np.concatenate([X_test_time_series, X_test_additional], axis=1)\n",
    "\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    no_improvement_epochs = 0\n",
    "    patience = 2\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        train_accuracy = accuracy_score(y_train_resampled, model.predict(X_train_resampled))\n",
    "        \n",
    "        if train_accuracy > best_score:\n",
    "            best_model = model\n",
    "            best_score = train_accuracy\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "        \n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    test_probabilities = best_model.predict_proba(X_test)\n",
    "    tp_list = []\n",
    "    tn_list = []\n",
    "    fp_list = []\n",
    "    fn_list = []\n",
    "    table_data = []\n",
    "    abstain_table_data = []\n",
    "    metrics_table_data = []\n",
    "\n",
    "    for fp_threshold in lambda_range:\n",
    "        for fn_threshold in lambda_range:\n",
    "            predictions, abstain_indices = classify_with_dual_reject(test_probabilities, fp_threshold, fn_threshold)\n",
    "            filtered_indices = [i for i in range(len(predictions)) if predictions[i] != -1]\n",
    "            y_test_filtered = y_test[filtered_indices]\n",
    "            predictions_filtered = predictions[filtered_indices]\n",
    "        \n",
    "            if len(predictions_filtered) > 0:\n",
    "                cm = confusion_matrix(y_test_filtered, predictions_filtered, labels=[0, 1])\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "            tp_list.append(tp)\n",
    "            tn_list.append(tn)\n",
    "            fp_list.append(fp)\n",
    "            fn_list.append(fn)\n",
    "            table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), tn, fp, fn, tp])\n",
    "            abstain_instances_info = [(idx, y_test[idx]) for idx in abstain_indices]\n",
    "            abstain_table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), abstain_instances_info])\n",
    "\n",
    "            accuracy = (accuracy_score(y_test_filtered, predictions_filtered) * 100) if len(y_test_filtered) > 0 else 0\n",
    "            precision = (precision_score(y_test_filtered, predictions_filtered, zero_division=0) * 100) if len(y_test_filtered) > 0 else 0\n",
    "            recall = (recall_score(y_test_filtered, predictions_filtered, zero_division=0) * 100) if len(y_test_filtered) > 0 else 0\n",
    "            f1 = (f1_score(y_test_filtered, predictions_filtered, zero_division=0) * 100) if len(y_test_filtered) > 0 else 0\n",
    "            specificity = ((tn / (tn + fp)) * 100) if (tn + fp) > 0 else 0\n",
    "\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['accuracy'].append(accuracy)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['precision'].append(precision)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['recall'].append(recall)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['f1'].append(f1)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['specificity'].append(specificity)\n",
    "            metrics_table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), f\"{accuracy:.2f}%\", f\"{precision:.2f}%\", f\"{recall:.2f}%\", f\"{f1:.2f}%\", f\"{specificity:.2f}%\"])\n",
    "\n",
    "            if fn == 0 and fn_threshold_to_remove_all_fn is None:\n",
    "                fn_threshold_to_remove_all_fn = fn_threshold\n",
    "            if fp == 0 and fp_threshold_to_remove_all_fp is None:\n",
    "                fp_threshold_to_remove_all_fp = fp_threshold\n",
    "\n",
    "            if cm.shape != (2, 2):\n",
    "                cm_padded = np.zeros((2, 2), dtype=int)\n",
    "                cm_padded[:cm.shape[0], :cm.shape[1]] = cm\n",
    "            else:\n",
    "                cm_padded = cm\n",
    "\n",
    "            x_labels = ['Normal', 'Abnormal']\n",
    "            y_labels = ['Abnormal', 'Normal']  # Reverse the order of y_labels\n",
    "            cm_reversed = cm_padded[::-1]\n",
    "            fig = ff.create_annotated_heatmap(z=cm_reversed, x=x_labels, y=y_labels, colorscale='Blues')\n",
    "            fig.update_layout(\n",
    "                title=f'Confusion Matrix, Fold {fold_number}, FP Lambda {fp_threshold:.2f}, FN Lambda {fn_threshold:.2f}',\n",
    "                xaxis=dict(title='Predicted labels', tickfont=dict(size=10)),\n",
    "                yaxis=dict(title='True labels', tickfont=dict(size=10)),\n",
    "                width=400,\n",
    "                height=300,\n",
    "                margin=dict(l=50, r=50, t=100, b=50)\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "    # Plot confusion matrix elements vs. lambda\n",
    "    for metric_list, label in zip([tp_list, tn_list, fp_list, fn_list], \n",
    "                                  ['True Positives (TP)', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)']):\n",
    "        fig = go.Figure()\n",
    "        for fp_threshold in lambda_range:\n",
    "            subset = [metric_list[i] for i in range(len(metric_list)) if table_data[i][0] == fp_threshold]\n",
    "            fig.add_trace(go.Scatter(x=lambda_range, y=subset, mode='lines+markers', name=f'FP Lambda={fp_threshold}'))\n",
    "        fig.update_layout(\n",
    "            title=f'Confusion Matrix Elements vs. FN Lambda Threshold ({label})',\n",
    "            xaxis_title='FN Lambda (Abstain Threshold)',\n",
    "            yaxis_title='Count',\n",
    "            width=800,\n",
    "            height=600\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    # Create a DataFrame for the table and display it\n",
    "    df_table_cm = pd.DataFrame(table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)', 'True Positives (TP)'])\n",
    "    fig_table_cm = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_table_cm.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_table_cm[col].tolist() for col in df_table_cm.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_table_cm.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_table_cm.show()\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df_table_cm.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Confusion_Matrix_Elements_Fold_{fold_number}.xlsx', index=False)\n",
    "  \n",
    "    # Create a DataFrame for abstain instances table and display it\n",
    "    df_abstain_table = pd.DataFrame(abstain_table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'Abstain Instances (Index, True Label)'])\n",
    "    fig_abstain_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_abstain_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_abstain_table[col].tolist() for col in df_abstain_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_abstain_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_abstain_table.show()\n",
    "\n",
    "    # Save the abstain instances DataFrame to an Excel file\n",
    "    df_abstain_table.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Instances_Fold_{fold_number}.xlsx', index=False)\n",
    "    \n",
    "    # Create a DataFrame for the performance metrics table and display it\n",
    "    df_metrics_table = pd.DataFrame(metrics_table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity'])\n",
    "    fig_metrics_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_metrics_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_metrics_table[col].tolist() for col in df_metrics_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_metrics_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_metrics_table.show()\n",
    "\n",
    "    # Save the performance metrics DataFrame to an Excel file\n",
    "    df_metrics_table.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Results_Metrics_Fold_{fold_number}.xlsx', index=False)\n",
    "\n",
    "    fold_number += 1\n",
    "\n",
    "# Calculate average metrics for each lambda combination across all folds\n",
    "avg_metrics_data = []\n",
    "for fp in lambda_range:\n",
    "    for fn in lambda_range:\n",
    "        avg_accuracy = np.mean(metrics_dict[(fp, fn)]['accuracy'])\n",
    "        avg_precision = np.mean(metrics_dict[(fp, fn)]['precision'])\n",
    "        avg_recall = np.mean(metrics_dict[(fp, fn)]['recall'])\n",
    "        avg_f1 = np.mean(metrics_dict[(fp, fn)]['f1'])\n",
    "        avg_specificity = np.mean(metrics_dict[(fp, fn)]['specificity'])\n",
    "    \n",
    "        avg_metrics_data.append([round(fp, 2), round(fn, 2), f\"{avg_accuracy:.2f}%\", f\"{avg_precision:.2f}%\", f\"{avg_recall:.2f}%\", f\"{avg_f1:.2f}%\", f\"{avg_specificity:.2f}%\"])\n",
    "\n",
    "# Create a DataFrame for average metrics and display it\n",
    "df_avg_metrics = pd.DataFrame(avg_metrics_data, columns=['FP Lambda', 'FN Lambda', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity'])\n",
    "fig_avg_metrics = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_avg_metrics.columns), fill_color='paleturquoise', align='left'),\n",
    "    cells=dict(values=[df_avg_metrics[col].tolist() for col in df_avg_metrics.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    ")])\n",
    "fig_avg_metrics.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "fig_avg_metrics.show()\n",
    "\n",
    "# Save the average metrics DataFrame to an Excel file\n",
    "df_avg_metrics.to_excel(f'{save_directory}/Average_Dual_Lambda_Metrics.xlsx', index=False)\n",
    "\n",
    "# Plot the average performance metrics vs. lambda\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Accuracy']], marker='o', linestyle='-', label='Average Accuracy')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Precision']], marker='o', linestyle='-', label='Average Precision')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Recall']], marker='o', linestyle='-', label='Average Recall')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average F1-score']], marker='o', linestyle='-', label='Average F1-score')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Specificity']], marker='o', linestyle='-', label='Average Specificity')\n",
    "plt.xlabel('FP Lambda (Abstain Threshold)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Average Performance Metrics vs. FP Lambda Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage metrics for each combination of FP and FN lambda across all folds have been saved to 'Average_Dual_Lambda_Metrics.xlsx'.\")\n",
    "print(f\"\\nFP threshold to remove all FP predictions: {fp_threshold_to_remove_all_fp}\")\n",
    "print(f\"FN threshold to remove all FN predictions: {fn_threshold_to_remove_all_fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f703b0-a233-4bb2-800c-8035a2bcd2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc229cf-ea9c-4ca4-96db-861e23b6cdf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2520318453.py, line 173)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 173\u001b[1;36m\u001b[0m\n\u001b[1;33m    )\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Step 1: Load Data from CSV file\n",
    "data_path = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\Data_Results\\FHR-dataset-CTUUHB\\combined_FHR_data_resampled_with_minutes.csv\"\n",
    "save_directory = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\Research\\FHRT\\PROJECT\\GitHub\\FHR_Project\\Codes\\ML_models\\Models\\All_Results\\RF_Results\\TwoAbstains\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Step 2: Run Your Analysis Code\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x == 1 else 1)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "X_time_series = df.iloc[:, :-15].values\n",
    "X_additional_features = df[['Age', 'Gravidity', 'Sex', 'Parity', 'Hypertension', 'Diabetes', 'Preeclampsia',\n",
    "                             'Liq. praecox', 'Pyrexia', 'Meconium', 'Presentation', 'Induced', 'I.stage',\n",
    "                             'NoProgress', 'II.stage']].values\n",
    "y = df['label'].values\n",
    "X = np.concatenate([X_time_series, X_additional_features], axis=1)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "lambda_range = np.arange(0.5, 0.95, 0.05)\n",
    "metrics_dict = {(fp, fn): {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []} for fp in lambda_range for fn in lambda_range}\n",
    "\n",
    "def classify_with_dual_reject(probabilities, fp_threshold, fn_threshold):\n",
    "    predictions = []\n",
    "    abstain_instances = []\n",
    "    for i, prob in enumerate(probabilities):\n",
    "        predicted_label = np.argmax(prob)\n",
    "        confidence = max(prob)\n",
    "        if (predicted_label == 1 and confidence < fp_threshold) or (predicted_label == 0 and confidence < fn_threshold):\n",
    "            predictions.append(-1)\n",
    "            abstain_instances.append(i)\n",
    "        else:\n",
    "            predictions.append(predicted_label)\n",
    "    return np.array(predictions), abstain_instances\n",
    "\n",
    "fold_number = 1\n",
    "fp_threshold_to_remove_all_fp = None\n",
    "fn_threshold_to_remove_all_fn = None\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    X_train_time_series = X_train[:, :-15]\n",
    "    X_train_additional = X_train[:, -15:]\n",
    "    X_test_time_series = X_test[:, :-15]\n",
    "    X_test_additional = X_test[:, -15:]\n",
    "\n",
    "    scaler_time_series = StandardScaler()\n",
    "    X_train_time_series = scaler_time_series.fit_transform(X_train_time_series)\n",
    "    X_test_time_series = scaler_time_series.transform(X_test_time_series)\n",
    "    scaler_additional = StandardScaler()\n",
    "    X_train_additional = scaler_additional.fit_transform(X_train_additional)\n",
    "    X_test_additional = scaler_additional.transform(X_test_additional)\n",
    "\n",
    "    X_train = np.concatenate([X_train_time_series, X_train_additional], axis=1)\n",
    "    X_test = np.concatenate([X_test_time_series, X_test_additional], axis=1)\n",
    "\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    no_improvement_epochs = 0\n",
    "    patience = 2\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        train_accuracy = accuracy_score(y_train_resampled, model.predict(X_train_resampled))\n",
    "        \n",
    "        if train_accuracy > best_score:\n",
    "            best_model = model\n",
    "            best_score = train_accuracy\n",
    "            no_improvement_epochs = 0\n",
    "        else:\n",
    "            no_improvement_epochs += 1\n",
    "        \n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    test_probabilities = best_model.predict_proba(X_test)\n",
    "    tp_list = []\n",
    "    tn_list = []\n",
    "    fp_list = []\n",
    "    fn_list = []\n",
    "    table_data = []\n",
    "    abstain_table_data = []\n",
    "    metrics_table_data = []\n",
    "\n",
    "    for fp_threshold in lambda_range:\n",
    "        for fn_threshold in lambda_range:\n",
    "            predictions, abstain_indices = classify_with_dual_reject(test_probabilities, fp_threshold, fn_threshold)\n",
    "            filtered_indices = [i for i in range(len(predictions)) if predictions[i] != -1]\n",
    "            y_test_filtered = y_test[filtered_indices]\n",
    "            predictions_filtered = predictions[filtered_indices]\n",
    "        \n",
    "            if len(predictions_filtered) > 0:\n",
    "                cm = confusion_matrix(y_test_filtered, predictions_filtered, labels=[0, 1])\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "            else:\n",
    "                tn, fp, fn, tp = 0, 0, 0, 0\n",
    "\n",
    "            tp_list.append(tp)\n",
    "            tn_list.append(tn)\n",
    "            fp_list.append(fp)\n",
    "            fn_list.append(fn)\n",
    "            table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), tn, fp, fn, tp])\n",
    "            abstain_instances_info = [(idx, y_test[idx]) for idx in abstain_indices]\n",
    "            abstain_table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), abstain_instances_info])\n",
    "\n",
    "            accuracy = (accuracy_score(y_test_filtered, predictions_filtered) * 100) if len(y_test_filtered) > 0 else 0\n",
    "            precision = (precision_score(y_test_filtered, predictions_filtered, zero_division=0) * 100) if len(y_test_filtered) > 0 else 0\n",
    "            recall = (recall_score(y_test_filtered, predictions_filtered, zero_division=0) * 100) if len(y_test_filtered) > 0 else 0\n",
    "            f1 = (f1_score(y_test_filtered, predictions_filtered, zero_division=0) * 100) if len(y_test_filtered) > 0 else 0\n",
    "            specificity = ((tn / (tn + fp)) * 100) if (tn + fp) > 0 else 0\n",
    "\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['accuracy'].append(accuracy)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['precision'].append(precision)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['recall'].append(recall)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['f1'].append(f1)\n",
    "            metrics_dict[(fp_threshold, fn_threshold)]['specificity'].append(specificity)\n",
    "            metrics_table_data.append([round(fp_threshold, 2), round(fn_threshold, 2), f\"{accuracy:.2f}%\", f\"{precision:.2f}%\", f\"{recall:.2f}%\", f\"{f1:.2f}%\", f\"{specificity:.2f}%\"])\n",
    "\n",
    "            if fn == 0 and fn_threshold_to_remove_all_fn is None:\n",
    "                fn_threshold_to_remove_all_fn = fn_threshold\n",
    "            if fp == 0 and fp_threshold_to_remove_all_fp is None:\n",
    "                fp_threshold_to_remove_all_fp = fp_threshold\n",
    "\n",
    "            if cm.shape != (2, 2):\n",
    "                cm_padded = np.zeros((2, 2), dtype=int)\n",
    "                cm_padded[:cm.shape[0], :cm.shape[1]] = cm\n",
    "            else:\n",
    "                cm_padded = cm\n",
    "\n",
    "            x_labels = ['Normal', 'Abnormal']\n",
    "            y_labels = ['Abnormal', 'Normal']  # Reverse the order of y_labels\n",
    "            cm_reversed = cm_padded[::-1]\n",
    "            fig = ff.create_annotated_heatmap(z=cm_reversed, x=x_labels, y=y_labels, colorscale='Blues')\n",
    "            fig.update_layout(\n",
    "                title=f'Confusion Matrix, Fold {fold_number}, FP Lambda {fp_threshold:.2f}, FN Lambda {fn_threshold:.2f}',\n",
    "                xaxis=dict(title='Predicted labels', tickfont=dict(size=10)),\n",
    "                yaxis=dict(title='True labels', tickfont=dict(size=10)),\n",
    "                width=400,\n",
    "                height=300,\n",
    "                margin=dict(l=50, r=50, t=100, b=50)\n",
    "            )\n",
    "            fig.show()\n",
    "\n",
    "    # Plot confusion matrix elements vs. lambda\n",
    "    for metric_list, label in zip([tp_list, tn_list, fp_list, fn_list], \n",
    "                                  ['True Positives (TP)', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)']):\n",
    "        fig = go.Figure()\n",
    "        for fp_threshold in lambda_range:\n",
    "            subset_indices = [i for i, data in enumerate(table_data) if data[0] == fp_threshold]\n",
    "            subset = [metric_list[i] for i in subset_indices]\n",
    "            fig.add_trace(go.Scatter(x=lambda_range, y=subset, mode='lines+markers', name=f'FP Lambda={fp_threshold}'))\n",
    "        fig.update_layout(\n",
    "            title=f'Confusion Matrix Elements vs. FN Lambda Threshold ({label})',\n",
    "            xaxis_title='FN Lambda (Abstain Threshold)',\n",
    "            yaxis_title='Count',\n",
    "            width=800,\n",
    "            height=600\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    # Create a DataFrame for the table and display it\n",
    "    df_table_cm = pd.DataFrame(table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'True Negatives (TN)', 'False Positives (FP)', 'False Negatives (FN)', 'True Positives (TP)'])\n",
    "    fig_table_cm = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_table_cm.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_table_cm[col].tolist() for col in df_table_cm.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_table_cm.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_table_cm.show()\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    df_table_cm.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Confusion_Matrix_Elements_Fold_{fold_number}.xlsx', index=False)\n",
    "  \n",
    "    # Create a DataFrame for abstain instances table and display it\n",
    "    df_abstain_table = pd.DataFrame(abstain_table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'Abstain Instances (Index, True Label)'])\n",
    "    fig_abstain_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_abstain_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_abstain_table[col].tolist() for col in df_abstain_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_abstain_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_abstain_table.show()\n",
    "\n",
    "    # Save the abstain instances DataFrame to an Excel file\n",
    "    df_abstain_table.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Instances_Fold_{fold_number}.xlsx', index=False)\n",
    "    \n",
    "    # Create a DataFrame for the performance metrics table and display it\n",
    "    df_metrics_table = pd.DataFrame(metrics_table_data, columns=['FP Lambda Threshold', 'FN Lambda Threshold', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity'])\n",
    "    fig_metrics_table = go.Figure(data=[go.Table(\n",
    "        header=dict(values=list(df_metrics_table.columns), fill_color='paleturquoise', align='left'),\n",
    "        cells=dict(values=[df_metrics_table[col].tolist() for col in df_metrics_table.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    "    )])\n",
    "    fig_metrics_table.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "    fig_metrics_table.show()\n",
    "\n",
    "    # Save the performance metrics DataFrame to an Excel file\n",
    "    df_metrics_table.to_excel(f'{save_directory}/Lambda_Dual_Abstain_Results_Metrics_Fold_{fold_number}.xlsx', index=False)\n",
    "\n",
    "    fold_number += 1\n",
    "\n",
    "# Calculate average metrics for each lambda combination across all folds\n",
    "avg_metrics_data = []\n",
    "for fp in lambda_range:\n",
    "    for fn in lambda_range:\n",
    "        avg_accuracy = np.mean(metrics_dict[(fp, fn)]['accuracy'])\n",
    "        avg_precision = np.mean(metrics_dict[(fp, fn)]['precision'])\n",
    "        avg_recall = np.mean(metrics_dict[(fp, fn)]['recall'])\n",
    "        avg_f1 = np.mean(metrics_dict[(fp, fn)]['f1'])\n",
    "        avg_specificity = np.mean(metrics_dict[(fp, fn)]['specificity'])\n",
    "    \n",
    "        avg_metrics_data.append([round(fp, 2), round(fn, 2), f\"{avg_accuracy:.2f}%\", f\"{avg_precision:.2f}%\", f\"{avg_recall:.2f}%\", f\"{avg_f1:.2f}%\", f\"{avg_specificity:.2f}%\"])\n",
    "\n",
    "# Create a DataFrame for average metrics and display it\n",
    "df_avg_metrics = pd.DataFrame(avg_metrics_data, columns=['FP Lambda', 'FN Lambda', 'Average Accuracy', 'Average Precision', 'Average Recall', 'Average F1-score', 'Average Specificity'])\n",
    "fig_avg_metrics = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(df_avg_metrics.columns), fill_color='paleturquoise', align='left'),\n",
    "    cells=dict(values=[df_avg_metrics[col].tolist() for col in df_avg_metrics.columns], fill=dict(color=['lavender', 'white']), align='left')\n",
    ")])\n",
    "fig_avg_metrics.update_layout(width=1000, height=500)  # Adjust the size as needed to fit the table and ensure all entries are visible\n",
    "fig_avg_metrics.show()\n",
    "\n",
    "# Save the average metrics DataFrame to an Excel file\n",
    "df_avg_metrics.to_excel(f'{save_directory}/Average_Dual_Lambda_Metrics.xlsx', index=False)\n",
    "\n",
    "# Plot the average performance metrics vs. lambda\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Accuracy']], marker='o', linestyle='-', label='Average Accuracy')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Precision']], marker='o', linestyle='-', label='Average Precision')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Recall']], marker='o', linestyle='-', label='Average Recall')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average F1-score']], marker='o', linestyle='-', label='Average F1-score')\n",
    "plt.plot(lambda_range, [float(x.rstrip('%')) for x in df_avg_metrics['Average Specificity']], marker='o', linestyle='-', label='Average Specificity')\n",
    "plt.xlabel('FP Lambda (Abstain Threshold)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Average Performance Metrics vs. FP Lambda Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAverage metrics for each combination of FP and FN lambda across all folds have been saved to 'Average_Dual_Lambda_Metrics.xlsx'.\")\n",
    "print(f\"\\nFP threshold to remove all FP predictions: {fp_threshold_to_remove_all_fp}\")\n",
    "print(f\"FN threshold to remove all FN predictions: {fn_threshold_to_remove_all_fn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91becf1-778b-44b1-94ad-0f78ce60a724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
